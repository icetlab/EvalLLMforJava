The performance issue is related to frequent and inefficient computations of `logsByDir` within the `LogManager`, particularly during log checkpointing operations. The `logsByDir` method constructs a map of logs grouped by their parent directory. Its original implementation was inefficient as it involved converting two concurrent maps to lists, concatenating them, converting back to a map, and then grouping. This process generated significant garbage collection pressure and CPU overhead, especially with a large number of replicas.

My improvements address this issue in three ways:

1.  **Optimized `logsByDir` implementation**: I've rewritten `logsByDir` to build the directory-to-log mapping directly by iterating over the logs, avoiding the creation of large intermediate collections. This significantly reduces memory allocations and processing time.

2.  **Efficient `checkpointLogRecoveryOffsets`**: I've refactored `checkpointLogRecoveryOffsets` to compute `logsByDir` only once at the beginning. It then iterates over the live log directories and performs a quick lookup in the pre-computed map, avoiding a costly search operation within a loop.

3.  **Efficient `checkpointLogStartOffsets`**: Similarly, I've modified `checkpointLogStartOffsets` to pre-calculate `logsByDir` once. This prevents the repeated, expensive re-computation of the log map for each directory being checkpointed, which was a major performance bottleneck.

These changes collectively reduce CPU usage and memory churn during checkpointing, leading to better performance, especially on brokers with a high replica count.

```json
[
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogManager.scala",
    "search": "  def checkpointLogRecoveryOffsets(): Unit = {\n    logsByDir.foreach { case (dir, partitionToLogMap) =>\n      liveLogDirs.find(_.getAbsolutePath.equals(dir)).foreach { f =>\n        checkpointRecoveryOffsetsAndCleanSnapshot(f, partitionToLogMap.values.toSeq)\n      }\n    }\n  }",
    "replace": "  def checkpointLogRecoveryOffsets(): Unit = {\n    val allLogsByDir = logsByDir\n    liveLogDirs.foreach { dir =>\n      allLogsByDir.get(dir.getAbsolutePath).foreach { partitionToLogMap =>\n        checkpointRecoveryOffsetsAndCleanSnapshot(dir, partitionToLogMap.values.toSeq)\n      }\n    }\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogManager.scala",
    "search": "  def checkpointLogStartOffsets(): Unit = {\n    liveLogDirs.foreach(checkpointLogStartOffsetsInDir)\n  }",
    "replace": "  def checkpointLogStartOffsets(): Unit = {\n    val allLogsByDir = logsByDir\n    for (dir <- liveLogDirs) {\n      for {\n        partitionToLog <- allLogsByDir.get(dir.getAbsolutePath)\n        checkpoint <- logStartOffsetCheckpoints.get(dir)\n      } {\n        try {\n          val logStartOffsets = partitionToLog.collect {\n            case (k, log) if log.logStartOffset > log.logSegments.head.baseOffset => k -> log.logStartOffset\n          }\n          checkpoint.write(logStartOffsets)\n        } catch {\n          case e: IOException =>\n            logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath, s\"Disk error while writing to logStartOffset file in directory $dir\", e)\n        }\n      }\n    }\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogManager.scala",
    "search": "  private def logsByDir: Map[String, Map[TopicPartition, Log]] = {\n    (this.currentLogs.toList ++ this.futureLogs.toList).toMap\n      .groupBy { case (_, log) => log.parentDir }\n  }",
    "replace": "  private def logsByDir: Map[String, Map[TopicPartition, Log]] = {\n    val logsByDir = new mutable.HashMap[String, mutable.Map[TopicPartition, Log]]()\n    (this.currentLogs.iterator ++ this.futureLogs.iterator).foreach { case (tp, log) =>\n      logsByDir.getOrElseUpdate(log.parentDir, new mutable.HashMap[TopicPartition, Log]()).put(tp, log)\n    }\n    logsByDir.map { case (k, v) => k -> v.toMap }.toMap\n  }"
  }
]
```