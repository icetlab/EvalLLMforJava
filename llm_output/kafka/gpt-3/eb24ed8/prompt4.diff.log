Iâ€™ve added Java-level buffering around the Zstd streams to batch up small reads/writes into larger chunks, reducing JNI overhead and internal buffer churn.

```json
[
  {
    "filepath": "../kafka/clients/src/main/java/org/apache/kafka/common/record/CompressionType.java",
    "search": "public OutputStream wrapForOutput(ByteBufferOutputStream buffer, byte messageVersion) {\n            try {\n                return (OutputStream) ZstdConstructors.OUTPUT.invoke(buffer);\n            } catch (Throwable e) {",
    "replace": "public OutputStream wrapForOutput(ByteBufferOutputStream buffer, byte messageVersion) {\n            try {\n                OutputStream zstdOut = (OutputStream) ZstdConstructors.OUTPUT.invoke(buffer);\n                // Buffer writes to reduce JNI calls\n                return new BufferedOutputStream(zstdOut, 16 * 1024);\n            } catch (Throwable e) {"
  },
  {
    "filepath": "../kafka/clients/src/main/java/org/apache/kafka/common/record/CompressionType.java",
    "search": "public InputStream wrapForInput(ByteBuffer buffer, byte messageVersion, BufferSupplier decompressionBufferSupplier) {\n            try {\n                return (InputStream) ZstdConstructors.INPUT.invoke(new ByteBufferInputStream(buffer));\n            } catch (Throwable e) {",
    "replace": "public InputStream wrapForInput(ByteBuffer buffer, byte messageVersion, BufferSupplier decompressionBufferSupplier) {\n            try {\n                InputStream zstdIn = (InputStream) ZstdConstructors.INPUT.invoke(new ByteBufferInputStream(buffer));\n                // Buffer reads to reduce JNI calls\n                return new BufferedInputStream(zstdIn, 16 * 1024);\n            } catch (Throwable e) {"
  }
]
```