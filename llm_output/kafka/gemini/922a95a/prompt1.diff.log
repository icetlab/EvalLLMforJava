My overall improvement strategy is to enhance the performance of the `Partition` class by optimizing frequently executed or critical-path operations. I will achieve this by:
1. Merging two separate loops over the remote replica list within the `makeLeader` method into a single loop, reducing redundant iterations during leadership changes.
2. Replacing an inefficient combination of `exists` and `dropWhile` calls with a single, more performant `span` operation for collection processing in `legacyFetchOffsetsForTimestamp`.
3. Simplifying a collection conversion in `makeFollower` to improve code consistency and remove a minor inefficiency.

These changes collectively contribute to better CPU utilization and reduced object allocation, particularly in scenarios involving leadership changes and offset fetching.

```json
[
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "      // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.\n      remoteReplicas.foreach { replica =>\n        val lastCaughtUpTimeMs = if (inSyncReplicaIds.contains(replica.brokerId)) curTimeMs else 0L\n        replica.resetLastCaughtUpTime(leaderEpochStartOffset, curTimeMs, lastCaughtUpTimeMs)\n      }\n\n      if (isNewLeader) {\n        // mark local replica as the leader after converting hw\n        leaderReplicaIdOpt = Some(localBrokerId)\n        // reset log end offset for remote replicas\n        remoteReplicas.foreach { replica =>\n          replica.updateFetchState(\n            followerFetchOffsetMetadata = LogOffsetMetadata.UnknownOffsetMetadata,\n            followerStartOffset = Log.UnknownOffset,\n            followerFetchTimeMs = 0L,\n            leaderEndOffset = Log.UnknownOffset,\n            lastSentHighwatermark = 0L)\n        }\n      }",
    "replace": "      // initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.\n      remoteReplicas.foreach { replica =>\n        val lastCaughtUpTimeMs = if (inSyncReplicaIds.contains(replica.brokerId)) curTimeMs else 0L\n        replica.resetLastCaughtUpTime(leaderEpochStartOffset, curTimeMs, lastCaughtUpTimeMs)\n\n        if (isNewLeader) {\n          replica.updateFetchState(\n            followerFetchOffsetMetadata = LogOffsetMetadata.UnknownOffsetMetadata,\n            followerStartOffset = Log.UnknownOffset,\n            followerFetchTimeMs = 0L,\n            leaderEndOffset = Log.UnknownOffset,\n            lastSentHighwatermark = 0L)\n        }\n      }\n\n      if (isNewLeader) {\n        // mark local replica as the leader after converting hw\n        leaderReplicaIdOpt = Some(localBrokerId)\n      }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "        assignment = partitionState.replicas.asScala.iterator.map(_.toInt).toSeq,",
    "replace": "        assignment = partitionState.replicas.asScala.map(_.toInt),"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "    if (!isFromConsumer) {\n      allOffsets\n    } else {\n      val hw = localLog.highWatermark\n      if (allOffsets.exists(_ > hw))\n        hw +: allOffsets.dropWhile(_ > hw)\n      else\n        allOffsets\n    }",
    "replace": "    if (!isFromConsumer) {\n      allOffsets\n    } else {\n      val hw = localLog.highWatermark\n      val (higher, lower) = allOffsets.span(_ > hw)\n      if (higher.nonEmpty)\n        hw +: lower\n      else\n        lower\n    }"
  }
]
```