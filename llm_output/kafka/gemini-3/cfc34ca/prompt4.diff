diff --git a/core/src/main/scala/kafka/log/LogManager.scala b/core/src/main/scala/kafka/log/LogManager.scala
index fc1e7195d7..ec74c5af94 100755
--- a/core/src/main/scala/kafka/log/LogManager.scala
+++ b/core/src/main/scala/kafka/log/LogManager.scala
@@ -470,7 +470,7 @@ class LogManager(logDirs: Seq[File],
         checkpointRecoveryOffsetsAndCleanSnapshot(dir, localLogsByDir.getOrElse(dir.toString, Map()).values.toSeq)
 
         debug(s"Updating log start offsets at $dir")
-        checkpointLogStartOffsetsInDir(dir)
+        checkpointLogStartOffsetsInDir(dir, localLogsByDir.get(dir.toString))
 
         // mark that the shutdown was clean by creating marker file
         debug(s"Writing clean shutdown marker at $dir")
@@ -580,7 +580,8 @@ class LogManager(logDirs: Seq[File],
    * to avoid exposing data that have been deleted by DeleteRecordsRequest
    */
   def checkpointLogStartOffsets(): Unit = {
-    liveLogDirs.foreach(checkpointLogStartOffsetsInDir)
+    val allLogsByDir = logsByDir
+    liveLogDirs.foreach(dir => checkpointLogStartOffsetsInDir(dir, allLogsByDir.get(dir.getAbsolutePath)))
   }
 
   /**
@@ -613,9 +614,9 @@ class LogManager(logDirs: Seq[File],
   /**
    * Checkpoint log start offset for all logs in provided directory.
    */
-  private def checkpointLogStartOffsetsInDir(dir: File): Unit = {
+  private def checkpointLogStartOffsetsInDir(dir: File, partitionToLogMapOption: Option[Map[TopicPartition, Log]]): Unit = {
     for {
-      partitionToLog <- logsByDir.get(dir.getAbsolutePath)
+      partitionToLog <- partitionToLogMapOption
       checkpoint <- logStartOffsetCheckpoints.get(dir)
     } {
       try {
@@ -877,7 +878,7 @@ class LogManager(logDirs: Seq[File],
         // Close the log, update checkpoint files, and enqueue this log to be deleted.
         sourceLog.close()
         checkpointRecoveryOffsetsAndCleanSnapshot(sourceLog.parentDirFile, ArrayBuffer.empty)
-        checkpointLogStartOffsetsInDir(sourceLog.parentDirFile)
+        checkpointLogStartOffsetsInDir(sourceLog.parentDirFile, logsByDir.get(sourceLog.parentDirFile.getAbsolutePath))
         addLogToBeDeleted(sourceLog)
       } catch {
         case e: KafkaStorageException =>
@@ -915,7 +916,7 @@ class LogManager(logDirs: Seq[File],
       }
       removedLog.renameDir(Log.logDeleteDirName(topicPartition))
       checkpointRecoveryOffsetsAndCleanSnapshot(removedLog.parentDirFile, ArrayBuffer.empty)
-      checkpointLogStartOffsetsInDir(removedLog.parentDirFile)
+      checkpointLogStartOffsetsInDir(removedLog.parentDirFile, logsByDir.get(removedLog.parentDirFile.getAbsolutePath))
       addLogToBeDeleted(removedLog)
       info(s"Log for partition ${removedLog.topicPartition} is renamed to ${removedLog.dir.getAbsolutePath} and is scheduled for deletion")
     } else if (offlineLogDirs.nonEmpty) {
@@ -1004,8 +1005,11 @@ class LogManager(logDirs: Seq[File],
    * Map of log dir to logs by topic and partitions in that dir
    */
   private def logsByDir: Map[String, Map[TopicPartition, Log]] = {
-    (this.currentLogs.toList ++ this.futureLogs.toList).toMap
-      .groupBy { case (_, log) => log.parentDir }
+    val builder = mutable.Map.empty[String, mutable.Map[TopicPartition, Log]]
+    (currentLogs.iterator ++ futureLogs.iterator).foreach { case (tp, log) =>
+      builder.getOrElseUpdate(log.parentDir, mutable.Map.empty) += (tp -> log)
+    }
+    builder.view.mapValues(_.toMap).toMap
   }
 
   // logDir should be an absolute path
