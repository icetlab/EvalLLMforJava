The primary performance improvement in this change is to replace Scala's `for` loops that iterate over Java collections with `while` loops using Java iterators directly. This avoids the overhead of creating Scala wrapper objects for each element, which is particularly beneficial in hot paths like record validation. By manually managing iteration and indexing, we reduce allocations and improve overall throughput.

```json
[
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "    for (batch <- records.batches.asScala) {\n      validateBatch(topicPartition, firstBatch, batch, origin, toMagicValue, brokerTopicStats)\n\n      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n        // we fail the batch if any record fails, so we stop appending if any record fails\n        if (recordErrors.isEmpty)\n          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n      }\n\n      processRecordErrors(recordErrors)\n    }",
    "replace": "    val batchIterator = records.batches.iterator\n    while (batchIterator.hasNext) {\n      val batch = batchIterator.next()\n      validateBatch(topicPartition, firstBatch, batch, origin, toMagicValue, brokerTopicStats)\n\n      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      val recordIterator = batch.iterator\n      var batchIndex = 0\n      while (recordIterator.hasNext) {\n        val record = recordIterator.next()\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n        // we fail the batch if any record fails, so we stop appending if any record fails\n        if (recordErrors.isEmpty)\n          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n        batchIndex += 1\n      }\n\n      processRecordErrors(recordErrors)\n    }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "    for (batch <- records.batches.asScala) {\n      validateBatch(topicPartition, firstBatch, batch, origin, magic, brokerTopicStats)\n\n      var maxBatchTimestamp = RecordBatch.NO_TIMESTAMP\n      var offsetOfMaxBatchTimestamp = -1L\n\n      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n\n        val offset = offsetCounter.getAndIncrement()\n        if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && record.timestamp > maxBatchTimestamp) {\n          maxBatchTimestamp = record.timestamp\n          offsetOfMaxBatchTimestamp = offset\n        }\n      }\n\n      processRecordErrors(recordErrors)\n\n      if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && maxBatchTimestamp > maxTimestamp) {\n        maxTimestamp = maxBatchTimestamp\n        offsetOfMaxTimestamp = offsetOfMaxBatchTimestamp\n      }\n\n      batch.setLastOffset(offsetCounter.value - 1)\n\n      if (batch.magic >= RecordBatch.MAGIC_VALUE_V2)\n        batch.setPartitionLeaderEpoch(partitionLeaderEpoch)\n\n      if (batch.magic > RecordBatch.MAGIC_VALUE_V0) {\n        if (timestampType == TimestampType.LOG_APPEND_TIME)\n          batch.setMaxTimestamp(TimestampType.LOG_APPEND_TIME, now)\n        else\n          batch.setMaxTimestamp(timestampType, maxBatchTimestamp)\n      }\n    }",
    "replace": "    val batchIterator = records.batches.iterator\n    while (batchIterator.hasNext) {\n      val batch = batchIterator.next()\n      validateBatch(topicPartition, firstBatch, batch, origin, magic, brokerTopicStats)\n\n      var maxBatchTimestamp = RecordBatch.NO_TIMESTAMP\n      var offsetOfMaxBatchTimestamp = -1L\n\n      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      val recordIterator = batch.iterator\n      var batchIndex = 0\n      while (recordIterator.hasNext) {\n        val record = recordIterator.next()\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n\n        val offset = offsetCounter.getAndIncrement()\n        if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && record.timestamp > maxBatchTimestamp) {\n          maxBatchTimestamp = record.timestamp\n          offsetOfMaxBatchTimestamp = offset\n        }\n        batchIndex += 1\n      }\n\n      processRecordErrors(recordErrors)\n\n      if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && maxBatchTimestamp > maxTimestamp) {\n        maxTimestamp = maxBatchTimestamp\n        offsetOfMaxTimestamp = offsetOfMaxBatchTimestamp\n      }\n\n      batch.setLastOffset(offsetCounter.value - 1)\n\n      if (batch.magic >= RecordBatch.MAGIC_VALUE_V2)\n        batch.setPartitionLeaderEpoch(partitionLeaderEpoch)\n\n      if (batch.magic > RecordBatch.MAGIC_VALUE_V0) {\n        if (timestampType == TimestampType.LOG_APPEND_TIME)\n          batch.setMaxTimestamp(TimestampType.LOG_APPEND_TIME, now)\n        else\n          batch.setMaxTimestamp(timestampType, maxBatchTimestamp)\n      }\n    }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "    val batches = records.batches.asScala\n    for (batch <- batches) {\n      validateBatch(topicPartition, firstBatch, batch, origin, toMagic, brokerTopicStats)\n      uncompressedSizeInBytes += AbstractRecords.recordBatchHeaderSizeInBytes(toMagic, batch.compressionType())\n\n      // if we are on version 2 and beyond, and we know we are going for in place assignment,\n      // then we can optimize the iterator to skip key / value / headers since they would not be used at all\n      val recordsIterator = if (inPlaceAssignment && firstBatch.magic >= RecordBatch.MAGIC_VALUE_V2)\n        batch.skipKeyValueIterator(BufferSupplier.NO_CACHING)\n      else\n        batch.streamingIterator(BufferSupplier.NO_CACHING)\n\n      try {\n        val recordErrors = new ArrayBuffer[ApiRecordError](0)\n        var batchIndex = 0\n        for (record <- recordsIterator.asScala) {\n          val expectedOffset = expectedInnerOffset.getAndIncrement()\n          val recordError = validateRecordCompression(batchIndex, record).orElse {\n            validateRecord(batch, topicPartition, record, batchIndex, now,\n              timestampType, timestampDiffMaxMs, compactedTopic, brokerTopicStats).orElse {\n              if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && toMagic > RecordBatch.MAGIC_VALUE_V0) {\n                if (record.timestamp > maxTimestamp)\n                  maxTimestamp = record.timestamp\n\n                // Some older clients do not implement the V1 internal offsets correctly.\n                // Historically the broker handled this by rewriting the batches rather\n                // than rejecting the request. We must continue this handling here to avoid\n                // breaking these clients.\n                if (record.offset != expectedOffset)\n                  inPlaceAssignment = false\n              }\n              None\n            }\n          }\n\n          recordError match {\n            case Some(e) => recordErrors += e\n            case None =>\n              uncompressedSizeInBytes += record.sizeInBytes()\n              validatedRecords += record\n          }\n         batchIndex += 1\n        }\n        processRecordErrors(recordErrors)\n      } finally {\n        recordsIterator.close()\n      }\n    }",
    "replace": "    val batchIterator = records.batches.iterator\n    while (batchIterator.hasNext) {\n      val batch = batchIterator.next()\n      validateBatch(topicPartition, firstBatch, batch, origin, toMagic, brokerTopicStats)\n      uncompressedSizeInBytes += AbstractRecords.recordBatchHeaderSizeInBytes(toMagic, batch.compressionType())\n\n      // if we are on version 2 and beyond, and we know we are going for in place assignment,\n      // then we can optimize the iterator to skip key / value / headers since they would not be used at all\n      val recordsIterator = if (inPlaceAssignment && firstBatch.magic >= RecordBatch.MAGIC_VALUE_V2)\n        batch.skipKeyValueIterator(BufferSupplier.NO_CACHING)\n      else\n        batch.streamingIterator(BufferSupplier.NO_CACHING)\n\n      try {\n        val recordErrors = new ArrayBuffer[ApiRecordError](0)\n        var batchIndex = 0\n        while (recordsIterator.hasNext) {\n          val record = recordsIterator.next()\n          val expectedOffset = expectedInnerOffset.getAndIncrement()\n          val recordError = validateRecordCompression(batchIndex, record).orElse {\n            validateRecord(batch, topicPartition, record, batchIndex, now,\n              timestampType, timestampDiffMaxMs, compactedTopic, brokerTopicStats).orElse {\n              if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && toMagic > RecordBatch.MAGIC_VALUE_V0) {\n                if (record.timestamp > maxTimestamp)\n                  maxTimestamp = record.timestamp\n\n                // Some older clients do not implement the V1 internal offsets correctly.\n                // Historically the broker handled this by rewriting the batches rather\n                // than rejecting the request. We must continue this handling here to avoid\n                // breaking these clients.\n                if (record.offset != expectedOffset)\n                  inPlaceAssignment = false\n              }\n              None\n            }\n          }\n\n          recordError match {\n            case Some(e) => recordErrors += e\n            case None =>\n              uncompressedSizeInBytes += record.sizeInBytes()\n              validatedRecords += record\n          }\n          batchIndex += 1\n        }\n        processRecordErrors(recordErrors)\n      } finally {\n        recordsIterator.close()\n      }\n    }"
  }
]
```