diff --git a/core/src/main/scala/kafka/log/LogValidator.scala b/core/src/main/scala/kafka/log/LogValidator.scala
index 48b9f57d9a..09a385b62c 100644
--- a/core/src/main/scala/kafka/log/LogValidator.scala
+++ b/core/src/main/scala/kafka/log/LogValidator.scala
@@ -234,16 +234,22 @@ private[log] object LogValidator extends Logging {
 
     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)
 
-    for (batch <- records.batches.asScala) {
+    val batchIterator = records.batches.iterator
+    while (batchIterator.hasNext) {
+      val batch = batchIterator.next()
       validateBatch(topicPartition, firstBatch, batch, origin, toMagicValue, brokerTopicStats)
 
       val recordErrors = new ArrayBuffer[ApiRecordError](0)
-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {
+      val recordIterator = batch.iterator
+      var batchIndex = 0
+      while (recordIterator.hasNext) {
+        val record = recordIterator.next()
         validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,
           timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)
         // we fail the batch if any record fails, so we stop appending if any record fails
         if (recordErrors.isEmpty)
           builder.appendWithOffset(offsetCounter.getAndIncrement(), record)
+        batchIndex += 1
       }
 
       processRecordErrors(recordErrors)
@@ -279,14 +285,19 @@ private[log] object LogValidator extends Logging {
 
     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)
 
-    for (batch <- records.batches.asScala) {
+    val batchIterator = records.batches.iterator
+    while (batchIterator.hasNext) {
+      val batch = batchIterator.next()
       validateBatch(topicPartition, firstBatch, batch, origin, magic, brokerTopicStats)
 
       var maxBatchTimestamp = RecordBatch.NO_TIMESTAMP
       var offsetOfMaxBatchTimestamp = -1L
 
       val recordErrors = new ArrayBuffer[ApiRecordError](0)
-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {
+      val recordIterator = batch.iterator
+      var batchIndex = 0
+      while (recordIterator.hasNext) {
+        val record = recordIterator.next()
         validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,
           timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)
 
@@ -295,6 +306,7 @@ private[log] object LogValidator extends Logging {
           maxBatchTimestamp = record.timestamp
           offsetOfMaxBatchTimestamp = offset
         }
+        batchIndex += 1
       }
 
       processRecordErrors(recordErrors)
@@ -390,8 +402,9 @@ private[log] object LogValidator extends Logging {
     if (sourceCodec == NoCompressionCodec && firstBatch.isControlBatch)
       inPlaceAssignment = true
 
-    val batches = records.batches.asScala
-    for (batch <- batches) {
+    val batchIterator = records.batches.iterator
+    while (batchIterator.hasNext) {
+      val batch = batchIterator.next()
       validateBatch(topicPartition, firstBatch, batch, origin, toMagic, brokerTopicStats)
       uncompressedSizeInBytes += AbstractRecords.recordBatchHeaderSizeInBytes(toMagic, batch.compressionType())
 
@@ -405,7 +418,8 @@ private[log] object LogValidator extends Logging {
       try {
         val recordErrors = new ArrayBuffer[ApiRecordError](0)
         var batchIndex = 0
-        for (record <- recordsIterator.asScala) {
+        while (recordsIterator.hasNext) {
+          val record = recordsIterator.next()
           val expectedOffset = expectedInnerOffset.getAndIncrement()
           val recordError = validateRecordCompression(batchIndex, record).orElse {
             validateRecord(batch, topicPartition, record, batchIndex, now,
