The performance issue is that `Partition.makeFollower()` can be blocked by ongoing log appends on a follower replica. This is due to both `makeFollower()` and `appendRecordsToFollowerOrFutureReplica()` acquiring locks on the same `leaderIsrUpdateLock`. `makeFollower()` takes a write lock, while follower appends take a read lock, causing contention.

To resolve this, I will introduce a new `ReentrantReadWriteLock` named `futureLogLock`. This new lock will specifically manage concurrency related to log modifications on follower replicas, particularly when a future log is involved for replica directory changes.

Here's the plan:
1.  A new `futureLogLock` will be added to the `Partition` class.
2.  Methods that modify the follower's log (`doAppendRecordsToFollowerOrFutureReplica`, `truncateTo`, `truncateFullyAndStartAt`) will now acquire a read lock on `futureLogLock` instead of `leaderIsrUpdateLock`. This decouples them from leader-centric operations like `makeFollower`.
3.  Methods that manage the lifecycle of the log and future log (`delete`, `removeFutureLocalReplica`, `maybeReplaceCurrentWithFutureReplica`) will acquire a write lock on `futureLogLock` in addition to `leaderIsrUpdateLock`. This ensures that log appends and truncations are safely synchronized with operations that might delete or replace the underlying log files, preventing race conditions and potential I/O errors.

By isolating the locking for follower log appends, `makeFollower()` will no longer be blocked, improving performance during leadership changes, such as a controlled shutdown.

```json
[
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "  // The read lock is only required when multiple reads are executed and needs to be in a consistent manner\n  private val leaderIsrUpdateLock = new ReentrantReadWriteLock",
    "replace": "  // The read lock is only required when multiple reads are executed and needs to be in a consistent manner\n  private val leaderIsrUpdateLock = new ReentrantReadWriteLock\n  // A lock that guards the future log. This is used to ensure that the log identity of the future-log-to-be-swapped\n  // is not changed during log directory reassignment.\n  private val futureLogLock = new ReentrantReadWriteLock"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "  def removeFutureLocalReplica(deleteFromLogDir: Boolean = true): Unit = {\n    inWriteLock(leaderIsrUpdateLock) {\n      futureLog = None\n      if (deleteFromLogDir)\n        logManager.asyncDelete(topicPartition, isFuture = true)\n    }\n  }",
    "replace": "  def removeFutureLocalReplica(deleteFromLogDir: Boolean = true): Unit = {\n    inWriteLock(leaderIsrUpdateLock) {\n      inWriteLock(futureLogLock) {\n        futureLog = None\n        if (deleteFromLogDir)\n          logManager.asyncDelete(topicPartition, isFuture = true)\n      }\n    }\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "      // The write lock is needed to make sure that while ReplicaAlterDirThread checks the LEO of the\n      // current replica, no other thread can update LEO of the current replica via log truncation or log append operation.\n      inWriteLock(leaderIsrUpdateLock) {\n        futureLog match {\n          case Some(futurePartitionLog) =>\n            if (log.exists(_.logEndOffset == futurePartitionLog.logEndOffset)) {\n              logManager.replaceCurrentWithFutureLog(topicPartition)\n              log = futureLog\n              removeFutureLocalReplica(false)\n              true\n            } else false\n          case None =>\n            // Future replica is removed by a non-ReplicaAlterLogDirsThread before this method is called\n            // In this case the partition should have been removed from state of the ReplicaAlterLogDirsThread\n            // Return false so that ReplicaAlterLogDirsThread does not have to remove this partition from the\n            // state again to avoid race condition\n            false\n        }\n      }",
    "replace": "      // The write lock is needed to make sure that while ReplicaAlterDirThread checks the LEO of the\n      // current replica, no other thread can update LEO of the current replica via log truncation or log append operation.\n      inWriteLock(leaderIsrUpdateLock) {\n        inWriteLock(futureLogLock) {\n          futureLog match {\n            case Some(futurePartitionLog) =>\n              if (log.exists(_.logEndOffset == futurePartitionLog.logEndOffset)) {\n                logManager.replaceCurrentWithFutureLog(topicPartition)\n                log = futureLog\n                removeFutureLocalReplica(false)\n                true\n              } else false\n            case None =>\n              // Future replica is removed by a non-ReplicaAlterLogDirsThread before this method is called\n              // In this case the partition should have been removed from state of the ReplicaAlterLogDirsThread\n              // Return false so that ReplicaAlterLogDirsThread does not have to remove this partition from the\n              // state again to avoid race condition\n              false\n          }\n        }\n      }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "  def delete(): Unit = {\n    // need to hold the lock to prevent appendMessagesToLeader() from hitting I/O exceptions due to log being deleted\n    inWriteLock(leaderIsrUpdateLock) {\n      remoteReplicasMap.clear()\n      assignmentState = SimpleAssignmentState(Seq.empty)\n      log = None\n      futureLog = None\n      inSyncReplicaIds = Set.empty\n      leaderReplicaIdOpt = None\n      leaderEpochStartOffsetOpt = None\n      Partition.removeMetrics(topicPartition)\n      logManager.asyncDelete(topicPartition)\n      if (logManager.getLog(topicPartition, isFuture = true).isDefined)\n        logManager.asyncDelete(topicPartition, isFuture = true)\n    }\n  }",
    "replace": "  def delete(): Unit = {\n    // need to hold the lock to prevent appendMessagesToLeader() from hitting I/O exceptions due to log being deleted\n    inWriteLock(leaderIsrUpdateLock) {\n      inWriteLock(futureLogLock) {\n        remoteReplicasMap.clear()\n        assignmentState = SimpleAssignmentState(Seq.empty)\n        log = None\n        futureLog = None\n        inSyncReplicaIds = Set.empty\n        leaderReplicaIdOpt = None\n        leaderEpochStartOffsetOpt = None\n        Partition.removeMetrics(topicPartition)\n        logManager.asyncDelete(topicPartition)\n        if (logManager.getLog(topicPartition, isFuture = true).isDefined)\n          logManager.asyncDelete(topicPartition, isFuture = true)\n      }\n    }\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "  private def doAppendRecordsToFollowerOrFutureReplica(records: MemoryRecords, isFuture: Boolean): Option[LogAppendInfo] = {\n    // The read lock is needed to handle race condition if request handler thread tries to\n    // remove future replica after receiving AlterReplicaLogDirsRequest.\n    inReadLock(leaderIsrUpdateLock) {\n      if (isFuture) {\n        // Note the replica may be undefined if it is removed by a non-ReplicaAlterLogDirsThread before\n        // this method is called\n        futureLog.map { _.appendAsFollower(records) }\n      } else {\n        // The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread\n        // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.\n        Some(localLogOrException.appendAsFollower(records))\n      }\n    }\n  }",
    "replace": "  private def doAppendRecordsToFollowerOrFutureReplica(records: MemoryRecords, isFuture: Boolean): Option[LogAppendInfo] = {\n    // The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread\n    // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.\n    inReadLock(futureLogLock) {\n      if (isFuture) {\n        // Note the replica may be undefined if it is removed by a non-ReplicaAlterLogDirsThread before\n        // this method is called\n        futureLog.map { _.appendAsFollower(records) }\n      } else {\n        Some(localLogOrException.appendAsFollower(records))\n      }\n    }\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "  def truncateTo(offset: Long, isFuture: Boolean): Unit = {\n    // The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread\n    // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.\n    inReadLock(leaderIsrUpdateLock) {\n      logManager.truncateTo(Map(topicPartition -> offset), isFuture = isFuture)\n    }\n  }",
    "replace": "  def truncateTo(offset: Long, isFuture: Boolean): Unit = {\n    // The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread\n    // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.\n    inReadLock(futureLogLock) {\n      logManager.truncateTo(Map(topicPartition -> offset), isFuture = isFuture)\n    }\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/cluster/Partition.scala",
    "search": "  def truncateFullyAndStartAt(newOffset: Long, isFuture: Boolean): Unit = {\n    // The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread\n    // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.\n    inReadLock(leaderIsrUpdateLock) {\n      logManager.truncateFullyAndStartAt(topicPartition, newOffset, isFuture = isFuture)\n    }\n  }",
    "replace": "  def truncateFullyAndStartAt(newOffset: Long, isFuture: Boolean): Unit = {\n    // The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread\n    // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.\n    inReadLock(futureLogLock) {\n      logManager.truncateFullyAndStartAt(topicPartition, newOffset, isFuture = isFuture)\n    }\n  }"
  }
]
```