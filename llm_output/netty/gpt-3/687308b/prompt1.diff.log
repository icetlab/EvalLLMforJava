Iâ€™ve optimized non-UTF8 component encoding to avoid allocating intermediate Strings and byte arrays on each segment. Instead, it uses `Charset.encode(...)` with a `ByteBuffer`, reducing garbage generation and improving throughput.

```json
[
  {
    "filepath": "../netty/codec-http/src/main/java/io/netty/handler/codec/http/QueryStringEncoder.java",
    "search": "byte[] bytes = new String(buf, 0, index).getBytes(charset);\n\n                for (byte b : bytes) {\n                    appendEncoded(b);\n                }",
    "replace": "java.nio.ByteBuffer bb = charset.encode(java.nio.CharBuffer.wrap(buf, 0, index));\n                while (bb.hasRemaining()) {\n                    appendEncoded(bb.get());\n                }"
  }
]
```