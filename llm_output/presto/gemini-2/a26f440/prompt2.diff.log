The primary performance issue with the current `TDigest` implementation, especially when dealing with many small distributions, lies within the `merge` method. The existing `merge` operation forces a complete compression of the source `TDigest` (`other`), which involves sorting and merging its buffered points into centroids. This process is computationally expensive, particularly when `other` contains only a few data points. Furthermore, it allocates intermediate `Centroid` objects and an `ArrayList` to hold them, which are then shuffled before being added to the target digest. This introduces unnecessary object creation overhead and processing time.

To optimize this, I will replace the `merge` method's implementation. The new approach will directly access the internal arrays of the `other` TDigest, bypassing the costly compression step and object allocations. It will combine both the unmerged points and the existing centroids from `other`, create a list of indices representing these points, shuffle these indices, and then add the points to the current digest in a randomized order. This preserves the desirable statistical property of random-order insertion (preventing bias) while being significantly more efficient by avoiding the premature merge and intermediate object allocations. This change is particularly beneficial in scenarios involving frequent merges of small T-Digests, as highlighted in the performance problem description.

```json
[
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/tdigest/TDigest.java",
    "search": "    public void merge(TDigest other)\n    {\n        checkArgument(other != null, \"Cannot merge with a null t-digest\");\n        checkArgument(this.publicCompression == other.getCompressionFactor(), \"TDigests must have the same compression, found (%s, %s)\", this.publicCompression,\n                other.getCompressionFactor());\n        List<Centroid> tmp = new ArrayList<>();\n        for (Centroid centroid : other.centroids()) {\n            tmp.add(centroid);\n        }\n\n        shuffle(tmp, gen);\n        for (Centroid centroid : tmp) {\n            add(centroid.getMean(), centroid.getWeight());\n        }\n    }",
    "replace": "    public void merge(TDigest other)\n    {\n        checkArgument(other != null, \"Cannot merge with a null t-digest\");\n        checkArgument(this.publicCompression == other.getCompressionFactor(), \"TDigests must have the same compression, found (%s, %s)\", this.publicCompression,\n                other.getCompressionFactor());\n\n        // The original implementation forced a merge on 'other', then collected the centroids into\n        // a list of objects, shuffled it, and then added the centroids. This is inefficient for\n        // small digests.\n        // This new implementation avoids the merge on 'other' and the object creation.\n        // It collects all points (unmerged and centroids) into a list of indices, shuffles\n        // the indices, and then adds the points in shuffled order to preserve accuracy.\n\n        int unmergedCount = other.tempUsed;\n        int centroidCount = other.activeCentroids;\n        int totalCount = unmergedCount + centroidCount;\n        if (totalCount == 0) {\n            return;\n        }\n\n        List<Integer> indices = new ArrayList<>(totalCount);\n        for (int i = 0; i < totalCount; i++) {\n            indices.add(i);\n        }\n        shuffle(indices, gen);\n\n        for (int index : indices) {\n            if (index < unmergedCount) {\n                // It's an unmerged point\n                add(other.tempMean[index], (long) other.tempWeight[index]);\n            }\n            else {\n                // It's a centroid\n                add(other.mean[index - unmergedCount], (long) other.weight[index - unmergedCount]);\n            }\n        }\n    }"
  }
]
```