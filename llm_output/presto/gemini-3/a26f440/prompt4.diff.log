My analysis indicates that the primary performance bottleneck for small T-Digest distributions occurs during the `merge` operation, particularly when many small, uncompressed digests are merged into a larger one. Each merge triggers a `compress` operation on the smaller digest, which involves sorting and processing its data points to form centroids, even if there are only a few points. This is computationally expensive and unnecessary for small distributions.

To address this, I will introduce an optimization within the `merge(TDigest other)` method. The improved logic will detect if the `other` digest contains only unmerged points (i.e., it's a "new" digest that hasn't been compressed yet). If so, instead of forcing a compression on `other`, it will directly transfer the raw data points from `other`'s internal buffer to the current digest's buffer. This bypasses the costly compression step on the small digest, significantly speeding up the process of merging many small distributions, while preserving the correctness of the final merged digest.

```json
[
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/tdigest/TDigest.java",
    "search": "    public void merge(TDigest other)\n    {\n        checkArgument(other != null, \"Cannot merge with a null t-digest\");\n        checkArgument(this.publicCompression == other.getCompressionFactor(), \"TDigests must have the same compression, found (%s, %s)\", this.publicCompression,\n                other.getCompressionFactor());\n        List<Centroid> tmp = new ArrayList<>();\n        for (Centroid centroid : other.centroids()) {\n            tmp.add(centroid);\n        }\n\n        shuffle(tmp, gen);\n        for (Centroid centroid : tmp) {\n            add(centroid.getMean(), centroid.getWeight());\n        }\n    }",
    "replace": "    public void merge(TDigest other)\n    {\n        checkArgument(other != null, \"Cannot merge with a null t-digest\");\n        checkArgument(this.publicCompression == other.getCompressionFactor(), \"TDigests must have the same compression, found (%s, %s)\", this.publicCompression,\n                other.getCompressionFactor());\n\n        // if the other digest has not been compressed, we can just add its raw points to our buffer\n        // this avoids the expensive compression step on a small digest\n        if (other.totalWeight == 0 && other.unmergedWeight > 0) {\n            for (int i = 0; i < other.tempUsed; i++) {\n                add(other.tempMean[i], (long) other.tempWeight[i]);\n            }\n            return;\n        }\n\n        List<Centroid> tmp = new ArrayList<>();\n        for (Centroid centroid : other.centroids()) {\n            tmp.add(centroid);\n        }\n\n        shuffle(tmp, gen);\n        for (Centroid centroid : tmp) {\n            add(centroid.getMean(), centroid.getWeight());\n        }\n    }"
  }
]
```