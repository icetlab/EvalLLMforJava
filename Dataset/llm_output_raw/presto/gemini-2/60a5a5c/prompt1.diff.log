My overall improvement strategy focuses on reducing object allocations in hot paths to lessen garbage collection pressure and improve overall throughput.

Specifically, I will make the following optimizations:
1.  In `PagesSerde`, I will cache and reuse `ByteBuffer` instances for compression and decompression. Since `PagesSerde` is not thread-safe, we can safely introduce state to avoid repeated allocations within the `serialize` and `deserialize` methods.
2.  In `PartitionedOutputOperator`, I will optimize the `PagePartitioner` which is central to the operator's logic.
    *   I will cache the `RunLengthEncodedBlock`s created for partition constants. These blocks were being recreated for every page, even when the position count remained the same. Caching them avoids unnecessary object creation.
    *   I will replace a Stream-based collection with a more performant, imperative loop using an `ArrayList` for serializing pages in the `flush` method. This is a common micro-optimization that can yield benefits in tight loops.

These changes are designed to be functionally equivalent to the original code while improving its performance characteristics.

```json
[
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/execution/buffer/PagesSerde.java",
    "search": "    private final Optional<SpillCipher> spillCipher;",
    "replace": "    private final Optional<SpillCipher> spillCipher;\n\n    private ByteBuffer compressionBuffer;\n    private ByteBuffer decompressionBuffer;"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/execution/buffer/PagesSerde.java",
    "search": "        if (compressor.isPresent()) {\n            ByteBuffer compressionBuffer = ByteBuffer.allocate(compressor.get().maxCompressedLength(uncompressedSize));\n            compressor.get().compress(slice.toByteBuffer(), compressionBuffer);\n            compressionBuffer.flip();\n            if ((((double) compressionBuffer.remaining()) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n                slice = Slices.wrappedBuffer(compressionBuffer);\n                markers = COMPRESSED.set(markers);\n            }\n        }",
    "replace": "        if (compressor.isPresent()) {\n            int maxCompressedLength = compressor.get().maxCompressedLength(uncompressedSize);\n            if (compressionBuffer == null || compressionBuffer.capacity() < maxCompressedLength) {\n                compressionBuffer = ByteBuffer.allocate(maxCompressedLength);\n            }\n            else {\n                compressionBuffer.clear();\n            }\n            compressor.get().compress(slice.toByteBuffer(), compressionBuffer);\n            compressionBuffer.flip();\n            if ((((double) compressionBuffer.remaining()) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n                slice = Slices.wrappedBuffer(compressionBuffer);\n                markers = COMPRESSED.set(markers);\n            }\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/execution/buffer/PagesSerde.java",
    "search": "        if (COMPRESSED.isSet(serializedPage.getPageCodecMarkers())) {\n            checkState(decompressor.isPresent(), \"Page is compressed, but decompressor is missing\");\n\n            int uncompressedSize = serializedPage.getUncompressedSizeInBytes();\n            ByteBuffer decompressionBuffer = ByteBuffer.allocate(uncompressedSize);\n\n            decompressor.get().decompress(slice.toByteBuffer(), decompressionBuffer);\n            decompressionBuffer.flip();\n            checkState(decompressionBuffer.remaining() == uncompressedSize);\n\n            slice = Slices.wrappedBuffer(decompressionBuffer);\n        }",
    "replace": "        if (COMPRESSED.isSet(serializedPage.getPageCodecMarkers())) {\n            checkState(decompressor.isPresent(), \"Page is compressed, but decompressor is missing\");\n\n            int uncompressedSize = serializedPage.getUncompressedSizeInBytes();\n            if (decompressionBuffer == null || decompressionBuffer.capacity() < uncompressedSize) {\n                decompressionBuffer = ByteBuffer.allocate(uncompressedSize);\n            }\n            else {\n                decompressionBuffer.clear();\n            }\n            decompressionBuffer.limit(uncompressedSize);\n\n            decompressor.get().decompress(slice.toByteBuffer(), decompressionBuffer);\n            decompressionBuffer.flip();\n            checkState(decompressionBuffer.remaining() == uncompressedSize);\n\n            slice = Slices.wrappedBuffer(decompressionBuffer);\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/PartitionedOutputOperator.java",
    "search": "import io.airlift.units.DataSize;\n\nimport java.util.List;",
    "replace": "import io.airlift.units.DataSize;\n\nimport java.util.ArrayList;\nimport java.util.List;"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/PartitionedOutputOperator.java",
    "search": "        private final AtomicLong pagesAdded = new AtomicLong();\n        private boolean hasAnyRowBeenReplicated;\n        private final OperatorContext operatorContext;",
    "replace": "        private final AtomicLong pagesAdded = new AtomicLong();\n        private boolean hasAnyRowBeenReplicated;\n        private final OperatorContext operatorContext;\n        private final Block[] rleBlocks;\n        private int rlePositionCount = -1;"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/PartitionedOutputOperator.java",
    "search": "            for (int i = 0; i < partitionCount; i++) {\n                pageBuilders[i] = PageBuilder.withMaxPageSize(pageSize, sourceTypes);\n            }",
    "replace": "            for (int i = 0; i < partitionCount; i++) {\n                pageBuilders[i] = PageBuilder.withMaxPageSize(pageSize, sourceTypes);\n            }\n            this.rleBlocks = new Block[partitionChannels.size()];"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/PartitionedOutputOperator.java",
    "search": "        private Page getPartitionFunctionArguments(Page page)\n        {\n            Block[] blocks = new Block[partitionChannels.size()];\n            for (int i = 0; i < blocks.length; i++) {\n                Optional<Block> partitionConstant = partitionConstants.get(i);\n                if (partitionConstant.isPresent()) {\n                    blocks[i] = new RunLengthEncodedBlock(partitionConstant.get(), page.getPositionCount());\n                }\n                else {\n                    blocks[i] = page.getBlock(partitionChannels.get(i));\n                }\n            }\n            return new Page(page.getPositionCount(), blocks);\n        }",
    "replace": "        private Page getPartitionFunctionArguments(Page page)\n        {\n            Block[] blocks = new Block[partitionChannels.size()];\n            boolean newRleBlocksNeeded = page.getPositionCount() != rlePositionCount;\n            if (newRleBlocksNeeded) {\n                rlePositionCount = page.getPositionCount();\n            }\n\n            for (int i = 0; i < blocks.length; i++) {\n                Optional<Block> partitionConstant = partitionConstants.get(i);\n                if (partitionConstant.isPresent()) {\n                    if (newRleBlocksNeeded) {\n                        rleBlocks[i] = new RunLengthEncodedBlock(partitionConstant.get(), page.getPositionCount());\n                    }\n                    blocks[i] = rleBlocks[i];\n                }\n                else {\n                    blocks[i] = page.getBlock(partitionChannels.get(i));\n                }\n            }\n            return new Page(page.getPositionCount(), blocks);\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/PartitionedOutputOperator.java",
    "search": "                    List<SerializedPage> serializedPages = splitPage(pagePartition, DEFAULT_MAX_PAGE_SIZE_IN_BYTES).stream()\n                            .map(serde::serialize)\n                            .collect(toImmutableList());\n\n                    outputBuffer.enqueue(operatorContext.getDriverContext().getLifespan(), partition, serializedPages);",
    "replace": "                    List<Page> pages = splitPage(pagePartition, DEFAULT_MAX_PAGE_SIZE_IN_BYTES);\n                    List<SerializedPage> serializedPages = new ArrayList<>(pages.size());\n                    for (Page p : pages) {\n                        serializedPages.add(serde.serialize(p));\n                    }\n                    outputBuffer.enqueue(operatorContext.getDriverContext().getLifespan(), partition, serializedPages);"
  }
]
```