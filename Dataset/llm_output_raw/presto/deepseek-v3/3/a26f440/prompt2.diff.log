## Explanation of Improvements

The performance issue focuses on optimizing T-Digest for small distributions. The current implementation has several inefficiencies when handling small numbers of data points:

1. **Unnecessary merging overhead**: The current implementation triggers merges based on buffer capacity, which can be inefficient for small distributions where we could avoid merging entirely until necessary.

2. **Excessive array operations**: For small distributions, we can store data more efficiently without the overhead of large fixed-size arrays.

3. **Early compression calls**: Methods like `getCdf()` and `getQuantile()` call `compress()` even when unmerged weight is zero, adding unnecessary overhead.

Here are the targeted optimizations:

1. **Lazy merging**: Only merge when absolutely necessary (when buffer is full or when explicitly requested via `compress()`)
2. **Small distribution optimization**: When total weight is small, use simpler data structures and avoid complex merging logic
3. **Avoid unnecessary compression**: Skip compression in `getCdf()` and `getQuantile()` when there are no pending values

```json
[
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/tdigest/TDigest.java",
    "search": "    public void add(double x, long w)\n    {\n        checkArgument(!Double.isNaN(x), \"Cannot add NaN to t-digest\");\n        checkArgument(w > 0L, \"weight must be > 0\");\n\n        if (tempUsed >= tempWeight.length - activeCentroids - 1) {\n            mergeNewValues();\n        }\n        int where = tempUsed++;\n        tempWeight[where] = w;\n        tempMean[where] = x;\n        unmergedWeight += w;\n        if (x < min) {\n            min = x;\n        }\n        if (x > max) {\n            max = x;\n        }\n    }",
    "replace": "    public void add(double x, long w)\n    {\n        checkArgument(!Double.isNaN(x), \"Cannot add NaN to t-digest\");\n        checkArgument(w > 0L, \"weight must be > 0\");\n\n        // For very small distributions, we can avoid merging overhead\n        if (totalWeight + unmergedWeight + w <= 10 && activeCentroids == 0) {\n            // Store directly in temp arrays without immediate merging\n            if (tempUsed >= tempWeight.length) {\n                mergeNewValues();\n            }\n            int where = tempUsed++;\n            tempWeight[where] = w;\n            tempMean[where] = x;\n            unmergedWeight += w;\n        } else {\n            if (tempUsed >= tempWeight.length - activeCentroids - 1) {\n                mergeNewValues();\n            }\n            int where = tempUsed++;\n            tempWeight[where] = w;\n            tempMean[where] = x;\n            unmergedWeight += w;\n        }\n        \n        if (x < min) {\n            min = x;\n        }\n        if (x > max) {\n            max = x;\n        }\n    }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/tdigest/TDigest.java",
    "search": "    public double getCdf(double x)\n    {\n        if (unmergedWeight > 0) {\n            compress();\n        }\n\n        if (activeCentroids == 0) {\n            return Double.NaN;\n        }\n        if (activeCentroids == 1) {\n            double width = max - min;\n            if (x < min) {\n                return 0;\n            }\n            if (x > max) {\n                return 1;\n            }\n            if (x - min <= width) {\n                // min and max are too close together to do any viable interpolation\n                return 0.5;\n            }\n            return (x - min) / (max - min);\n        }",
    "replace": "    public double getCdf(double x)\n    {\n        // Only compress if we have pending values AND we have enough data to benefit from compression\n        if (unmergedWeight > 0 && totalWeight + unmergedWeight > 100) {\n            compress();\n        }\n\n        if (activeCentroids == 0) {\n            // If we have unmerged values but didn't compress, handle them directly\n            if (unmergedWeight > 0) {\n                return computeCdfFromTemp(x);\n            }\n            return Double.NaN;\n        }\n        if (activeCentroids == 1) {\n            double width = max - min;\n            if (x < min) {\n                return 0;\n            }\n            if (x > max) {\n                return 1;\n            }\n            if (x - min <= width) {\n                // min and max are too close together to do any viable interpolation\n                return 0.5;\n            }\n            return (x - min) / (max - min);\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/tdigest/TDigest.java",
    "search": "    public double getQuantile(double q)\n    {\n        checkArgument(q >= 0 && q <= 1, \"q should be in [0,1], got %s\", q);\n        if (unmergedWeight > 0) {\n            compress();\n        }\n\n        if (activeCentroids == 0) {\n            return Double.NaN;\n        }\n        else if (activeCentroids == 1) {\n            return mean[0];\n        }",
    "replace": "    public double getQuantile(double q)\n    {\n        checkArgument(q >= 0 && q <= 1, \"q should be in [0,1], got %s\", q);\n        // Only compress if we have pending values AND we have enough data to benefit from compression\n        if (unmergedWeight > 0 && totalWeight + unmergedWeight > 100) {\n            compress();\n        }\n\n        if (activeCentroids == 0) {\n            // If we have unmerged values but didn't compress, handle them directly\n            if (unmergedWeight > 0) {\n                return computeQuantileFromTemp(q);\n            }\n            return Double.NaN;\n        }\n        else if (activeCentroids == 1) {\n            return mean[0];\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/tdigest/TDigest.java",
    "search": "    private void mergeNewValues()\n    {\n        mergeNewValues(false, compression);\n    }",
    "replace": "    private void mergeNewValues()\n    {\n        // For very small distributions, use simpler merging logic\n        if (totalWeight + unmergedWeight <= 10) {\n            mergeNewValuesSmall();\n        } else {\n            mergeNewValues(false, compression);\n        }\n    }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/tdigest/TDigest.java",
    "search": "    private void mergeNewValues(boolean force, double compression)\n    {\n        if (unmergedWeight == 0) {\n            return;\n        }\n\n        if (force || unmergedWeight > 0) {\n            // note that we run the merge in reverse every other merge to avoid left-to-right bias in merging\n            merge(tempMean, tempWeight, tempUsed, order, unmergedWeight, mergeCount % 2 == 1, compression);\n            mergeCount++;\n            tempUsed = 0;\n            unmergedWeight = 0;\n        }\n    }",
    "replace": "    private void mergeNewValues(boolean force, double compression)\n    {\n        if (unmergedWeight == 0) {\n            return;\n        }\n\n        if (force || unmergedWeight > 0) {\n            // For small numbers of points, use optimized path\n            if (tempUsed <= 5 && activeCentroids == 0) {\n                // Direct insertion sort for very small datasets\n                for (int i = 0; i < tempUsed; i++) {\n                    mean[activeCentroids] = tempMean[i];\n                    weight[activeCentroids] = tempWeight[i];\n                    activeCentroids++;\n                    totalWeight += tempWeight[i];\n                }\n                // Sort the small array\n                for (int i = 0; i < activeCentroids; i++) {\n                    for (int j = i + 1; j < activeCentroids; j++) {\n                        if (mean[i] > mean[j]) {\n                            double tempMeanVal = mean[i];\n                            double tempWeightVal = weight[i];\n                            mean[i] = mean[j];\n                            weight[i] = weight[j];\n                            mean[j] = tempMeanVal;\n                            weight[j] = tempWeightVal;\n                        }\n                    }\n                }\n            } else {\n                // note that we run the merge in reverse every other merge to avoid left-to-right bias in merging\n                merge(tempMean, tempWeight, tempUsed, order, unmergedWeight, mergeCount % 2 == 1, compression);\n            }\n            mergeCount++;\n            tempUsed = 0;\n            unmergedWeight = 0;\n        }\n    }\n\n    private void mergeNewValuesSmall()\n    {\n        if (unmergedWeight == 0) {\n            return;\n        }\n\n        // Simple merge for small distributions\n        for (int i = 0; i < tempUsed; i++) {\n            boolean merged = false;\n            for (int j = 0; j < activeCentroids; j++) {\n                if (Math.abs(tempMean[i] - mean[j]) < 1e-10) {\n                    // Merge with existing centroid\n                    double newWeight = weight[j] + tempWeight[i];\n                    mean[j] = mean[j] + (tempMean[i] - mean[j]) * tempWeight[i] / newWeight;\n                    weight[j] = newWeight;\n                    merged = true;\n                    break;\n                }\n            }\n            if (!merged) {\n                // Add as new centroid\n                mean[activeCentroids] = tempMean[i];\n                weight[activeCentroids] = tempWeight[i];\n                activeCentroids++;\n            }\n            totalWeight += tempWeight[i];\n        }\n        \n        // Sort centroids\n        for (int i = 0; i < activeCentroids; i++) {\n            for (int j = i + 1; j < activeCentroids; j++) {\n                if (mean[i] > mean[j]) {\n                    double tempMeanVal = mean[i];\n                    double tempWeightVal = weight[i];\n                    mean[i] = mean[j];\n                    weight[i] = weight[j];\n                    mean[j] = tempMeanVal;\n                    weight[j] = tempWeightVal;\n                }\n            }\n        }\n        \n        tempUsed = 0;\n        unmergedWeight = 0;\n    }\n\n    private double computeCdfFromTemp(double x)\n    {\n        // Simple CDF computation from unmerged temp values\n        if (tempUsed == 0) {\n            return Double.NaN;\n        }\n        \n        int lessCount = 0;\n        double lessWeight = 0;\n        double equalWeight = 0;\n        \n        for (int i = 0; i < tempUsed; i++) {\n            if (tempMean[i] < x) {\n                lessWeight += tempWeight[i];\n                lessCount++;\n            } else if (tempMean[i] == x) {\n                equalWeight += tempWeight[i];\n            }\n        }\n        \n        if (lessCount == 0 && equalWeight == 0) {\n            return 0;\n        }\n        if (lessCount == tempUsed) {\n            return 1;\n        }\n        \n        return (lessWeight + equalWeight / 2) / (totalWeight + unmergedWeight);\n    }\n\n    private double computeQuantileFromTemp(double q)\n    {\n        // Simple quantile computation from unmerged temp values\n        if (tempUsed == 0) {\n            return Double.NaN;\n        }\n        \n        double targetWeight = q * (totalWeight + unmergedWeight);\n        double[] sortedMeans = new double[tempUsed];\n        double[] sortedWeights = new double[tempUsed];\n        \n        // Copy and sort\n        System.arraycopy(tempMean, 0, sortedMeans, 0, tempUsed);\n        System.arraycopy(tempWeight, 0, sortedWeights, 0, tempUsed);\n        \n        for (int i = 0; i < tempUsed; i++) {\n            for (int j = i + 1; j < tempUsed; j++) {\n                if (sortedMeans[i] > sortedMeans[j]) {\n                    double tempMeanVal = sortedMeans[i];\n                    double tempWeightVal = sortedWeights[i];\n                    sortedMeans[i] = sortedMeans[j];\n                    sortedWeights[i] = sortedWeights[j];\n                    sortedMeans[j] = tempMeanVal;\n                    sortedWeights[j] = tempWeightVal;\n                }\n            }\n        }\n        \n        double cumulativeWeight = 0;\n        for (int i = 0; i < tempUsed; i++) {\n            if (cumulativeWeight + sortedWeights[i] >= targetWeight) {\n                return sortedMeans[i];\n            }\n            cumulativeWeight += sortedWeights[i];\n        }\n        \n        return sortedMeans[tempUsed - 1];\n    }"
  }
]
```