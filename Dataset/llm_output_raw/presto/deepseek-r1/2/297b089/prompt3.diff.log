Looking at the `SliceDirectSelectiveStreamReader` class, I can identify several performance improvements. The main issues are:

1. **Unnecessary batch mode calculation**: The `useBatchMode()` method always returns `true`, making the condition check and complex logic in `prepareForNextRead` and `read` methods unnecessary overhead.
2. **Redundant calculations**: Some calculations are done repeatedly in hot paths.
3. **Inefficient loops**: Some loops can be optimized for better performance.
4. **Redundant null checks**: Some null checks can be simplified.

Here are the proposed improvements:

```json
[
  {
    "filepath": "../presto/presto-orc/src/main/java/com/facebook/presto/orc/reader/SliceDirectSelectiveStreamReader.java",
    "search": "    private boolean useBatchMode(int positionCount, int totalPositionCount)\n    {\n        return true;\n        // maxCodePointCount < 0 means it's unbounded varchar VARCHAR.\n        // If the types are VARCHAR(N) or CHAR(N), the length of the string need to be calculated and truncated.\n//        if (lengthStream == null || maxCodePointCount >= 0) {\n//            return false;\n//        }\n//\n//        double inputFilterRate = (double) (totalPositionCount - positionCount) / totalPositionCount;\n//        if (filter == null) {  // readNoFilter\n//            // When there is no filter, batch mode performs better for almost all inputFilterRate.\n//            // But to limit data buffer size, we enable it for the range of [0.0f, 0.5f]\n//            if (inputFilterRate >= 0.0f && inputFilterRate <= 0.5f) {\n//                return true;\n//            }\n//\n//            return false;\n//        }\n//        else { // readWithFilter\n//            // When there is filter, batch mode performs better for almost all inputFilterRate except when inputFilterRate is around 0.1f.\n//            // To limit data buffer size, we enable it for the range of [0.0f, 0.05f] and [0.15f, 0.5f]\n//            if (inputFilterRate >= 0.0f && inputFilterRate <= 0.05f || inputFilterRate >= 0.15f && inputFilterRate <= 0.5f) {\n//                return true;\n//            }\n//\n//            return false;\n//        }\n    }",
    "replace": "    private boolean useBatchMode(int positionCount, int totalPositionCount)\n    {\n        // Always use batch mode for optimal performance\n        return true;\n    }"
  },
  {
    "filepath": "../presto/presto-orc/src/main/java/com/facebook/presto/orc/reader/SliceDirectSelectiveStreamReader.java",
    "search": "    private int prepareForNextRead(int positionCount, int[] positions)\n            throws IOException\n    {\n        lengthIndex = 0;\n        outputPositionCount = 0;\n\n        int totalLength = 0;\n        int maxLength = 0;\n        int dataLength = 0;\n\n        int totalPositions = positions[positionCount - 1] + 1;\n        int nullCount = 0;\n        if (presentStream != null) {\n            isNullVector = ensureCapacity(isNullVector, totalPositions);\n            nullCount = presentStream.getUnsetBits(totalPositions, isNullVector);\n        }\n\n        if (lengthStream != null) {\n            int nonNullCount = totalPositions - nullCount;\n            lengthVector = ensureCapacity(lengthVector, nonNullCount);\n            lengthStream.nextIntVector(nonNullCount, lengthVector, 0);\n\n            if (useBatchMode(positionCount, totalPositions)) {\n                for (int i = 0; i < nonNullCount; i++) {\n                    totalLength += lengthVector[i];\n                    maxLength = Math.max(maxLength, lengthVector[i]);\n                }\n            }\n            else {\n                int positionIndex = 0;\n                int lengthIndex = 0;\n                for (int i = 0; i < totalPositions; i++) {\n                    boolean isNotNull = nullCount == 0 || !isNullVector[i];\n                    if (i == positions[positionIndex]) {\n                        if (isNotNull) {\n                            totalLength += lengthVector[lengthIndex];\n                            maxLength = Math.max(maxLength, lengthVector[lengthIndex]);\n                            lengthIndex++;\n                        }\n                        positionIndex++;\n                    }\n                    else if (isNotNull) {\n                        lengthIndex++;\n                    }\n                }\n            }\n\n            // TODO Do not throw if outputRequired == false\n            if (totalLength > ONE_GIGABYTE) {\n                throw new GenericInternalException(\n                        format(\"Values in column \\\"%s\\\" are too large to process for Presto. %s column values are larger than 1GB [%s]\",\n                                streamDescriptor.getFieldName(), positionCount,\n                                streamDescriptor.getOrcDataSourceId()));\n            }\n        }\n\n        if (outputRequired) {\n            if (presentStream != null && nullsAllowed) {\n                nulls = ensureCapacity(nulls, positionCount);\n            }\n            dataLength = totalLength;\n            data = ensureCapacity(data, totalLength);\n            offsets = ensureCapacity(offsets, totalPositions + 1);\n        }\n        else {\n            if (useBatchMode(positionCount, totalPositions)) {\n                dataLength = totalLength;\n                if (filter != null) {\n                    offsets = ensureCapacity(offsets, totalPositions + 1);\n                }\n            }\n            else {\n                dataLength = maxLength;\n            }\n\n            data = ensureCapacity(data, dataLength);\n        }\n\n        dataAsSlice = Slices.wrappedBuffer(data);\n        return dataLength;\n    }",
    "replace": "    private int prepareForNextRead(int positionCount, int[] positions)\n            throws IOException\n    {\n        lengthIndex = 0;\n        outputPositionCount = 0;\n\n        int totalLength = 0;\n        int dataLength = 0;\n\n        int totalPositions = positions[positionCount - 1] + 1;\n        int nullCount = 0;\n        if (presentStream != null) {\n            isNullVector = ensureCapacity(isNullVector, totalPositions);\n            nullCount = presentStream.getUnsetBits(totalPositions, isNullVector);\n        }\n\n        if (lengthStream != null) {\n            int nonNullCount = totalPositions - nullCount;\n            lengthVector = ensureCapacity(lengthVector, nonNullCount);\n            lengthStream.nextIntVector(nonNullCount, lengthVector, 0);\n\n            // Since useBatchMode always returns true, we only need the batch mode logic\n            for (int i = 0; i < nonNullCount; i++) {\n                totalLength += lengthVector[i];\n            }\n\n            // TODO Do not throw if outputRequired == false\n            if (totalLength > ONE_GIGABYTE) {\n                throw new GenericInternalException(\n                        format(\"Values in column \\\"%s\\\" are too large to process for Presto. %s column values are larger than 1GB [%s]\",\n                                streamDescriptor.getFieldName(), positionCount,\n                                streamDescriptor.getOrcDataSourceId()));\n            }\n        }\n\n        if (outputRequired) {\n            if (presentStream != null && nullsAllowed) {\n                nulls = ensureCapacity(nulls, positionCount);\n            }\n            dataLength = totalLength;\n            data = ensureCapacity(data, totalLength);\n            offsets = ensureCapacity(offsets, totalPositions + 1);\n        }\n        else {\n            dataLength = totalLength;\n            if (filter != null) {\n                offsets = ensureCapacity(offsets, totalPositions + 1);\n            }\n            data = ensureCapacity(data, dataLength);\n        }\n\n        dataAsSlice = Slices.wrappedBuffer(data);\n        return dataLength;\n    }"
  },
  {
    "filepath": "../presto/presto-orc/src/main/java/com/facebook/presto/orc/reader/SliceDirectSelectiveStreamReader.java",
    "search": "        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n            if (position > streamPosition) {\n                skipData(streamPosition, position - streamPosition);\n                streamPosition = position;\n            }\n\n            int offset = offsets[i];\n            if (presentStream != null && isNullVector[position]) {\n                if (offsets != null) {\n                    offsets[i + 1] = offset;\n                }\n                nulls[i] = true;\n            }\n            else {\n                int length = lengthVector[lengthIndex];\n                int truncatedLength = 0;\n                if (length > 0) {\n                    dataStream.next(data, offset, offset + length);\n                    truncatedLength = computeTruncatedLength(dataAsSlice, offset, length, maxCodePointCount, isCharType);\n                }\n                offsets[i + 1] = offset + truncatedLength;\n                lengthIndex++;\n                if (presentStream != null) {\n                    nulls[i] = false;\n                }\n            }\n            streamPosition++;\n        }",
    "replace": "        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n            if (position > streamPosition) {\n                skipData(streamPosition, position - streamPosition);\n                streamPosition = position;\n            }\n\n            int offset = offsets[i];\n            boolean isNull = presentStream != null && isNullVector[position];\n            if (isNull) {\n                offsets[i + 1] = offset;\n                nulls[i] = true;\n            }\n            else {\n                int length = lengthVector[lengthIndex];\n                int truncatedLength = 0;\n                if (length > 0) {\n                    dataStream.next(data, offset, offset + length);\n                    truncatedLength = computeTruncatedLength(dataAsSlice, offset, length, maxCodePointCount, isCharType);\n                }\n                offsets[i + 1] = offset + truncatedLength;\n                lengthIndex++;\n                if (presentStream != null) {\n                    nulls[i] = false;\n                }\n            }\n            streamPosition++;\n        }"
  },
  {
    "filepath": "../presto/presto-orc/src/main/java/com/facebook/presto/orc/reader/SliceDirectSelectiveStreamReader.java",
    "search": "        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n            if (position > streamPosition) {\n                skipData(streamPosition, position - streamPosition);\n                streamPosition = position;\n            }\n\n            int offset = outputRequired ? offsets[outputPositionCount] : 0;\n            if (presentStream != null && isNullVector[position]) {\n                if ((nonDeterministicFilter && filter.testNull()) || nullsAllowed) {\n                    if (outputRequired) {\n                        offsets[outputPositionCount + 1] = offset;\n                        nulls[outputPositionCount] = true;\n                    }\n                    outputPositions[outputPositionCount] = position;\n                    outputPositionCount++;\n                }\n            }\n            else {\n                int length = lengthVector[lengthIndex];\n                int dataOffset = outputRequired ? offset : 0;\n                if (filter.testLength(length)) {\n                    if (dataStream != null) {\n                        dataStream.skip(dataToSkip);\n                        dataToSkip = 0;\n                        dataStream.next(data, dataOffset, dataOffset + length);\n                        if (filter.testBytes(data, dataOffset, length)) {\n                            if (outputRequired) {\n                                int truncatedLength = computeTruncatedLength(dataAsSlice, dataOffset, length, maxCodePointCount, isCharType);\n                                offsets[outputPositionCount + 1] = offset + truncatedLength;\n                                if (nullsAllowed && presentStream != null) {\n                                    nulls[outputPositionCount] = false;\n                                }\n                            }\n                            outputPositions[outputPositionCount] = position;\n                            outputPositionCount++;\n                        }\n                    }\n                    else {\n                        assert length == 0;\n                        if (filter.testBytes(\"\".getBytes(), 0, 0)) {\n                            if (outputRequired) {\n                                offsets[outputPositionCount + 1] = offset;\n                                if (nullsAllowed && presentStream != null) {\n                                    nulls[outputPositionCount] = false;\n                                }\n                            }\n                            outputPositions[outputPositionCount] = position;\n                            outputPositionCount++;\n                        }\n                    }\n                }\n                else {\n                    dataToSkip += length;\n                }\n                lengthIndex++;\n            }\n\n            streamPosition++;",
    "replace": "        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n            if (position > streamPosition) {\n                skipData(streamPosition, position - streamPosition);\n                streamPosition = position;\n            }\n\n            int offset = outputRequired ? offsets[outputPositionCount] : 0;\n            boolean isNull = presentStream != null && isNullVector[position];\n            if (isNull) {\n                if ((nonDeterministicFilter && filter.testNull()) || nullsAllowed) {\n                    if (outputRequired) {\n                        offsets[outputPositionCount + 1] = offset;\n                        nulls[outputPositionCount] = true;\n                    }\n                    outputPositions[outputPositionCount] = position;\n                    outputPositionCount++;\n                }\n            }\n            else {\n                int length = lengthVector[lengthIndex];\n                int dataOffset = outputRequired ? offset : 0;\n                if (filter.testLength(length)) {\n                    if (dataStream != null) {\n                        dataStream.skip(dataToSkip);\n                        dataToSkip = 0;\n                        dataStream.next(data, dataOffset, dataOffset + length);\n                        if (filter.testBytes(data, dataOffset, length)) {\n                            if (outputRequired) {\n                                int truncatedLength = computeTruncatedLength(dataAsSlice, dataOffset, length, maxCodePointCount, isCharType);\n                                offsets[outputPositionCount + 1] = offset + truncatedLength;\n                                if (nullsAllowed && presentStream != null) {\n                                    nulls[outputPositionCount] = false;\n                                }\n                            }\n                            outputPositions[outputPositionCount] = position;\n                            outputPositionCount++;\n                        }\n                    }\n                    else {\n                        assert length == 0;\n                        if (filter.testBytes(\"\".getBytes(), 0, 0)) {\n                            if (outputRequired) {\n                                offsets[outputPositionCount + 1] = offset;\n                                if (nullsAllowed && presentStream != null) {\n                                    nulls[outputPositionCount] = false;\n                                }\n                            }\n                            outputPositions[outputPositionCount] = position;\n                            outputPositionCount++;\n                        }\n                    }\n                }\n                else {\n                    dataToSkip += length;\n                }\n                lengthIndex++;\n            }\n\n            streamPosition++;"
  },
  {
    "filepath": "../presto/presto-orc/src/main/java/com/facebook/presto/orc/reader/SliceDirectSelectiveStreamReader.java",
    "search": "    // No nulls\n    private int evaluateFilter(int[] positions, int positionCount)\n    {\n        int positionsIndex = 0;\n        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n            if (filter.testLength(lengthVector[position])) {\n                outputPositions[positionsIndex++] = position;  // compact positions on the fly\n            }\n        }\n\n        int filteredPositionCount = 0;\n        if (positionsIndex > 0) {\n            if (dataStream == null) {\n                // The length check has passed and there is no need to run testBytes because there is no data\n                filteredPositionCount = positionsIndex;\n            }\n            else {\n                int totalPositionCount = outputPositions[positionsIndex - 1] + 1;\n                convertLengthVectorToOffsetVector(lengthVector, totalPositionCount, offsets);\n                filteredPositionCount = testBytes(outputPositions, positionsIndex);\n            }\n        }\n\n        return filteredPositionCount;\n    }",
    "replace": "    // No nulls\n    private int evaluateFilter(int[] positions, int positionCount)\n    {\n        int positionsIndex = 0;\n        // Pre-calculate offsets if we have dataStream\n        boolean hasDataStream = dataStream != null;\n        int totalPositionCount = hasDataStream ? positions[positionCount - 1] + 1 : 0;\n        \n        if (hasDataStream) {\n            convertLengthVectorToOffsetVector(lengthVector, totalPositionCount, offsets);\n        }\n        \n        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n            if (filter.testLength(lengthVector[position])) {\n                if (hasDataStream) {\n                    int dataOffset = offsets[position];\n                    int length = offsets[position + 1] - dataOffset;\n                    if (filter.testBytes(data, dataOffset, length)) {\n                        outputPositions[positionsIndex++] = position;\n                    }\n                } else {\n                    outputPositions[positionsIndex++] = position;\n                }\n            }\n        }\n\n        return positionsIndex;\n    }"
  },
  {
    "filepath": "../presto/presto-orc/src/main/java/com/facebook/presto/orc/reader/SliceDirectSelectiveStreamReader.java",
    "search": "    private int testBytes(int[] positions, int positionCount)\n    {\n        int positionsIndex = 0;\n        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n\n            int dataOffset = offsets[position];\n            int length = offsets[position + 1] - dataOffset;\n            if (filter.testBytes(data, dataOffset, length)) {\n                positions[positionsIndex++] = position;\n            }\n        }\n        return positionsIndex;\n    }",
    "replace": "    // This method is no longer used after optimizing evaluateFilter\n    private int testBytes(int[] positions, int positionCount)\n    {\n        int positionsIndex = 0;\n        for (int i = 0; i < positionCount; i++) {\n            int position = positions[i];\n\n            int dataOffset = offsets[position];\n            int length = offsets[position + 1] - dataOffset;\n            if (filter.testBytes(data, dataOffset, length)) {\n                positions[positionsIndex++] = position;\n            }\n        }\n        return positionsIndex;\n    }"
  }
]
```

**Explanation of improvements:**

1. **Simplified `useBatchMode()` method**: Removed commented-out complex logic and left only the constant `return true` since that's the current behavior. This reduces method complexity and avoids unnecessary calculations.

2. **Optimized `prepareForNextRead()` method**: 
   - Removed the `maxLength` variable since it's only used in the non-batch path (which is never taken)
   - Removed the else branch for non-batch mode since `useBatchMode()` always returns true
   - Simplified logic for when `outputRequired` is false

3. **Optimized loops in `readNoFilter()` and `readWithFilter()`**:
   - Extracted `isNull` check to a local variable to avoid repeated null checks
   - This reduces the number of null checks in hot loops

4. **Optimized `evaluateFilter()` method**:
   - Combined the length check and bytes check in a single loop
   - Pre-calculated offsets once outside the loop instead of calling `convertLengthVectorToOffsetVector` separately
   - Eliminated the separate `testBytes()` call for better cache locality

These changes should improve performance by:
- Reducing redundant calculations
- Improving loop efficiency
- Reducing null checks in hot paths
- Better cache locality by combining operations