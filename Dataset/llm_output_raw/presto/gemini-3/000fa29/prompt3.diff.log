The overall improvement is to optimize the row size calculation in the `OptimizedPartitionedOutputOperator`. In the original implementation, the serialized size of each row was calculated independently for each destination partition. This is inefficient, especially when rows are replicated to multiple partitions, as it leads to redundant computations.

The proposed change centralizes this calculation. I will modify the `PagePartitioner` to compute the serialized size for every row in the input `Page` just once. These pre-calculated sizes are then passed down to each `PartitionBuffer`. Each `PartitionBuffer` will then simply look up the sizes for the rows assigned to it, avoiding re-computation. This significantly reduces CPU usage, particularly in scenarios with many partitions or when row replication is active.

This involves:
1.  Adding fields to `PagePartitioner` to store the page-level row sizes and a helper array for indexing.
2.  Introducing new logic in `PagePartitioner.partitionPage` to perform the centralized size calculation.
3.  Updating `PartitionBuffer.appendData` to use these pre-calculated sizes instead of computing them.
4.  Removing the now-redundant `populateSerializedRowSizes` method from `PartitionBuffer`.
5.  Adjusting the `calculateNextBatchSize` method to align with the new approach.

These changes are expected to improve the performance of partitioning data, especially for complex data types and high partition counts.

```json
[
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        private final DecodedBlockNode[] decodedBlocks;",
    "replace": "        private final DecodedBlockNode[] decodedBlocks;\n\n        private int[] pageSerializedRowSizes;\n        private int[] allPositions;"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "            // Decode the page just once. The decoded blocks will be fed to each PartitionBuffer object to set up AbstractBlockEncodingBuffer.\n            for (int i = 0; i < decodedBlocks.length; i++) {\n                decodedBlocks[i] = decodeBlock(flattener, blockLeaseCloser, page.getBlock(i));\n            }\n\n            // Copy the data to their destination partitions and flush when the buffer is full.\n            for (int i = 0; i < partitionBuffers.length; i++) {\n                partitionBuffers[i].appendData(decodedBlocks, fixedWidthRowSize, variableWidthChannels, outputBuffer);\n            }",
    "replace": "            // Decode the page just once. The decoded blocks will be fed to each PartitionBuffer object to set up AbstractBlockEncodingBuffer.\n            for (int i = 0; i < decodedBlocks.length; i++) {\n                decodedBlocks[i] = decodeBlock(flattener, blockLeaseCloser, page.getBlock(i));\n            }\n\n            // Calculate serialized row sizes once per page\n            if (variableWidthChannels.isEmpty()) {\n                pageSerializedRowSizes = null;\n            }\n            else {\n                pageSerializedRowSizes = ensureCapacity(pageSerializedRowSizes, positionCount, SMALL, INITIALIZE);\n\n                allPositions = ensureCapacity(allPositions, positionCount);\n                for (int i = 0; i < positionCount; i++) {\n                    allPositions[i] = i;\n                }\n\n                for (int channel : variableWidthChannels) {\n                    BlockEncodingBuffer sizeCalculator = createBlockEncodingBuffers(decodedBlocks[channel]);\n                    sizeCalculator.setupDecodedBlocksAndPositions(decodedBlocks[channel], allPositions, positionCount);\n                    sizeCalculator.accumulateSerializedRowSizes(pageSerializedRowSizes);\n                }\n\n                if (fixedWidthRowSize > 0) {\n                    for (int i = 0; i < positionCount; i++) {\n                        pageSerializedRowSizes[i] += fixedWidthRowSize;\n                    }\n                }\n            }\n\n            // Copy the data to their destination partitions and flush when the buffer is full.\n            for (int i = 0; i < partitionBuffers.length; i++) {\n                partitionBuffers[i].appendData(decodedBlocks, fixedWidthRowSize, variableWidthChannels.isEmpty(), outputBuffer, pageSerializedRowSizes);\n            }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        private void appendData(DecodedBlockNode[] decodedBlocks, int fixedWidthRowSize, List<Integer> variableWidthChannels, OutputBuffer outputBuffer)\n        {\n            if (decodedBlocks.length != channelCount) {\n                throw new IllegalArgumentException(format(\"Unexpected number of decoded blocks %d. It should be %d.\", decodedBlocks.length, channelCount));\n            }\n\n            if (positionCount == 0) {\n                return;\n            }\n\n            if (channelCount == 0) {\n                bufferedRowCount += positionCount;\n                return;\n            }\n\n            initializeBlockEncodingBuffers(decodedBlocks);\n\n            for (int i = 0; i < channelCount; i++) {\n                blockEncodingBuffers[i].setupDecodedBlocksAndPositions(decodedBlocks[i], positions, positionCount);\n            }\n\n            populateSerializedRowSizes(fixedWidthRowSize, variableWidthChannels);\n\n            // Due to the limitation of buffer size, we append the data batch by batch\n            int offset = 0;\n            do {\n                int batchSize = calculateNextBatchSize(fixedWidthRowSize, variableWidthChannels, offset);\n\n                for (int i = 0; i < channelCount; i++) {\n                    blockEncodingBuffers[i].setNextBatch(offset, batchSize);\n                    blockEncodingBuffers[i].appendDataInBatch();\n                }\n\n                bufferedRowCount += batchSize;\n                offset += batchSize;\n\n                if (bufferFull) {\n                    flush(outputBuffer);\n                    bufferFull = false;\n                }\n            }\n            while (offset < positionCount);\n        }",
    "replace": "        private void appendData(DecodedBlockNode[] decodedBlocks, int fixedWidthRowSize, boolean hasNoVariableWidth, OutputBuffer outputBuffer, int[] pageSerializedRowSizes)\n        {\n            if (decodedBlocks.length != channelCount) {\n                throw new IllegalArgumentException(format(\"Unexpected number of decoded blocks %d. It should be %d.\", decodedBlocks.length, channelCount));\n            }\n\n            if (positionCount == 0) {\n                return;\n            }\n\n            if (channelCount == 0) {\n                bufferedRowCount += positionCount;\n                return;\n            }\n\n            initializeBlockEncodingBuffers(decodedBlocks);\n\n            for (int i = 0; i < channelCount; i++) {\n                blockEncodingBuffers[i].setupDecodedBlocksAndPositions(decodedBlocks[i], positions, positionCount);\n            }\n\n            if (!hasNoVariableWidth) {\n                serializedRowSizes = ensureCapacity(serializedRowSizes, positionCount);\n                for (int i = 0; i < positionCount; i++) {\n                    serializedRowSizes[i] = pageSerializedRowSizes[positions[i]];\n                }\n            }\n\n            // Due to the limitation of buffer size, we append the data batch by batch\n            int offset = 0;\n            do {\n                int batchSize = calculateNextBatchSize(fixedWidthRowSize, hasNoVariableWidth, offset);\n\n                for (int i = 0; i < channelCount; i++) {\n                    blockEncodingBuffers[i].setNextBatch(offset, batchSize);\n                    blockEncodingBuffers[i].appendDataInBatch();\n                }\n\n                bufferedRowCount += batchSize;\n                offset += batchSize;\n\n                if (bufferFull) {\n                    flush(outputBuffer);\n                    bufferFull = false;\n                }\n            }\n            while (offset < positionCount);\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        /**\n         * Calculate the row sizes in bytes and write them to serializedRowSizes.\n         */\n        private void populateSerializedRowSizes(int fixedWidthRowSize, List<Integer> variableWidthChannels)\n        {\n            if (variableWidthChannels.isEmpty()) {\n                return;\n            }\n\n            serializedRowSizes = ensureCapacity(serializedRowSizes, positionCount, SMALL, INITIALIZE);\n\n            for (int i : variableWidthChannels) {\n                blockEncodingBuffers[i].accumulateSerializedRowSizes(serializedRowSizes);\n            }\n\n            for (int i = 0; i < positionCount; i++) {\n                serializedRowSizes[i] += fixedWidthRowSize;\n            }\n        }",
    "replace": ""
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        private int calculateNextBatchSize(int fixedWidthRowSize, List<Integer> variableWidthChannels, int startPosition)\n        {\n            int bytesRemaining = capacity - getSerializedBuffersSizeInBytes();\n\n            if (variableWidthChannels.isEmpty()) {\n                int maxPositionsFit = max(bytesRemaining / fixedWidthRowSize, 1);\n                if (maxPositionsFit <= positionCount - startPosition) {\n                    bufferFull = true;\n                    return maxPositionsFit;\n                }\n                return positionCount - startPosition;\n            }\n\n            verify(serializedRowSizes != null);\n            for (int i = startPosition; i < positionCount; i++) {\n                bytesRemaining -= serializedRowSizes[i];\n\n                if (bytesRemaining <= 0) {\n                    bufferFull = true;\n                    return max(i - startPosition, 1);\n                }\n            }\n\n            return positionCount - startPosition;\n        }",
    "replace": "        private int calculateNextBatchSize(int fixedWidthRowSize, boolean hasNoVariableWidth, int startPosition)\n        {\n            int bytesRemaining = capacity - getSerializedBuffersSizeInBytes();\n\n            if (hasNoVariableWidth) {\n                int maxPositionsFit = fixedWidthRowSize > 0 ? max(bytesRemaining / fixedWidthRowSize, 1) : positionCount - startPosition;\n                if (maxPositionsFit <= positionCount - startPosition) {\n                    bufferFull = true;\n                    return maxPositionsFit;\n                }\n                return positionCount - startPosition;\n            }\n\n            verify(serializedRowSizes != null);\n            for (int i = startPosition; i < positionCount; i++) {\n                bytesRemaining -= serializedRowSizes[i];\n\n                if (bytesRemaining <= 0) {\n                    bufferFull = true;\n                    return max(i - startPosition, 1);\n                }\n            }\n\n            return positionCount - startPosition;\n        }"
  }
]
```