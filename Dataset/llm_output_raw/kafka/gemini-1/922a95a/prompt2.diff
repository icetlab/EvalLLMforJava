diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala
index 9cfd99e8e5..75b1637622 100755
--- a/core/src/main/scala/kafka/cluster/Partition.scala
+++ b/core/src/main/scala/kafka/cluster/Partition.scala
@@ -203,6 +203,7 @@ class Partition(val topicPartition: TopicPartition,
   private val remoteReplicasMap = new Pool[Int, Replica]
   // The read lock is only required when multiple reads are executed and needs to be in a consistent manner
   private val leaderIsrUpdateLock = new ReentrantReadWriteLock
+  private val futureLogLock = new ReentrantReadWriteLock
   private var zkVersion: Int = LeaderAndIsr.initialZKVersion
   @volatile private var leaderEpoch: Int = LeaderAndIsr.initialLeaderEpoch - 1
   // start offset for 'leaderEpoch' above (leader epoch of the current leader for this partition),
@@ -427,6 +428,7 @@ class Partition(val topicPartition: TopicPartition,
     if (futureReplicaLEO.contains(localReplicaLEO)) {
       // The write lock is needed to make sure that while ReplicaAlterDirThread checks the LEO of the
       // current replica, no other thread can update LEO of the current replica via log truncation or log append operation.
+      inWriteLock(futureLogLock) {
         inWriteLock(leaderIsrUpdateLock) {
           futureLog match {
             case Some(futurePartitionLog) =>
@@ -444,6 +446,7 @@ class Partition(val topicPartition: TopicPartition,
               false
           }
         }
+      }
     } else false
   }
 
@@ -913,14 +916,16 @@ class Partition(val topicPartition: TopicPartition,
   private def doAppendRecordsToFollowerOrFutureReplica(records: MemoryRecords, isFuture: Boolean): Option[LogAppendInfo] = {
     // The read lock is needed to handle race condition if request handler thread tries to
     // remove future replica after receiving AlterReplicaLogDirsRequest.
-    inReadLock(leaderIsrUpdateLock) {
     if (isFuture) {
+      inReadLock(leaderIsrUpdateLock) {
         // Note the replica may be undefined if it is removed by a non-ReplicaAlterLogDirsThread before
         // this method is called
         futureLog.map { _.appendAsFollower(records) }
+      }
     } else {
       // The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread
       // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.
+      inReadLock(futureLogLock) {
         Some(localLogOrException.appendAsFollower(records))
       }
     }
@@ -1128,9 +1133,15 @@ class Partition(val topicPartition: TopicPartition,
   def truncateTo(offset: Long, isFuture: Boolean): Unit = {
     // The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread
     // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.
+    if (isFuture) {
       inReadLock(leaderIsrUpdateLock) {
         logManager.truncateTo(Map(topicPartition -> offset), isFuture = isFuture)
       }
+    } else {
+      inReadLock(futureLogLock) {
+        logManager.truncateTo(Map(topicPartition -> offset), isFuture = isFuture)
+      }
+    }
   }
 
   /**
@@ -1142,9 +1153,15 @@ class Partition(val topicPartition: TopicPartition,
   def truncateFullyAndStartAt(newOffset: Long, isFuture: Boolean): Unit = {
     // The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread
     // is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica.
+    if (isFuture) {
       inReadLock(leaderIsrUpdateLock) {
         logManager.truncateFullyAndStartAt(topicPartition, newOffset, isFuture = isFuture)
       }
+    } else {
+      inReadLock(futureLogLock) {
+        logManager.truncateFullyAndStartAt(topicPartition, newOffset, isFuture = isFuture)
+      }
+    }
   }
 
   /**
