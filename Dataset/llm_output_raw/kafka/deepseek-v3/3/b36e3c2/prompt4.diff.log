## Explanation

The performance issue is that `zipWithIndex` causes tuple allocation in hot paths. In the `LogValidator.scala` file, there are several loops that use `zipWithIndex` to iterate over records while tracking their index. Each call to `zipWithIndex` creates a tuple `(Record, Int)` for every element, which adds unnecessary object allocation overhead in performance-critical code paths.

We need to replace these `zipWithIndex` usages with manual index tracking using a mutable variable or a simple counter. This will eliminate the tuple allocations and improve performance.

Here are the specific changes:

1. In `convertAndAssignOffsetsNonCompressed`: Replace `batch.asScala.view.zipWithIndex` with a manual index counter.
2. In `assignOffsetsNonCompressed`: Replace `batch.asScala.view.zipWithIndex` with a manual index counter.
3. In `validateMessagesAndAssignOffsetsCompressed`: Replace the implicit `zipWithIndex` pattern with an explicit counter.

```json
[
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n        // we fail the batch if any record fails, so we stop appending if any record fails\n        if (recordErrors.isEmpty)\n          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n      }",
    "replace": "      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      var batchIndex = 0\n      for (record <- batch.asScala) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n        // we fail the batch if any record fails, so we stop appending if any record fails\n        if (recordErrors.isEmpty)\n          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n        batchIndex += 1\n      }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n\n        val offset = offsetCounter.getAndIncrement()\n        if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && record.timestamp > maxBatchTimestamp) {\n          maxBatchTimestamp = record.timestamp\n          offsetOfMaxBatchTimestamp = offset\n        }\n      }",
    "replace": "      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      var batchIndex = 0\n      for (record <- batch.asScala) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n\n        val offset = offsetCounter.getAndIncrement()\n        if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && record.timestamp > maxBatchTimestamp) {\n          maxBatchTimestamp = record.timestamp\n          offsetOfMaxBatchTimestamp = offset\n        }\n        batchIndex += 1\n      }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "      try {\n        val recordErrors = new ArrayBuffer[ApiRecordError](0)\n        var batchIndex = 0\n        for (record <- recordsIterator.asScala) {\n          val expectedOffset = expectedInnerOffset.getAndIncrement()\n          val recordError = validateRecordCompression(batchIndex, record).orElse {\n            validateRecord(batch, topicPartition, record, batchIndex, now,\n              timestampType, timestampDiffMaxMs, compactedTopic, brokerTopicStats).orElse {\n              if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && toMagic > RecordBatch.MAGIC_VALUE_V0) {\n                if (record.timestamp > maxTimestamp)\n                  maxTimestamp = record.timestamp\n\n                // Some older clients do not implement the V1 internal offsets correctly.\n                // Historically the broker handled this by rewriting the batches rather\n                // than rejecting the request. We must continue this handling here to avoid\n                // breaking these clients.\n                if (record.offset != expectedOffset)\n                  inPlaceAssignment = false\n              }\n              None\n            }\n          }\n\n          recordError match {\n            case Some(e) => recordErrors += e\n            case None =>\n              uncompressedSizeInBytes += record.sizeInBytes()\n              validatedRecords += record\n          }\n         batchIndex += 1\n        }\n        processRecordErrors(recordErrors)\n      } finally {\n        recordsIterator.close()\n      }",
    "replace": "      try {\n        val recordErrors = new ArrayBuffer[ApiRecordError](0)\n        var batchIndex = 0\n        for (record <- recordsIterator.asScala) {\n          val expectedOffset = expectedInnerOffset.getAndIncrement()\n          val recordError = validateRecordCompression(batchIndex, record).orElse {\n            validateRecord(batch, topicPartition, record, batchIndex, now,\n              timestampType, timestampDiffMaxMs, compactedTopic, brokerTopicStats).orElse {\n              if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && toMagic > RecordBatch.MAGIC_VALUE_V0) {\n                if (record.timestamp > maxTimestamp)\n                  maxTimestamp = record.timestamp\n\n                // Some older clients do not implement the V1 internal offsets correctly.\n                // Historically the broker handled this by rewriting the batches rather\n                // than rejecting the request. We must continue this handling here to avoid\n                // breaking these clients.\n                if (record.offset != expectedOffset)\n                  inPlaceAssignment = false\n              }\n              None\n            }\n          }\n\n          recordError match {\n            case Some(e) => recordErrors += e\n            case None =>\n              uncompressedSizeInBytes += record.sizeInBytes()\n              validatedRecords += record\n          }\n          batchIndex += 1\n        }\n        processRecordErrors(recordErrors)\n      } finally {\n        recordsIterator.close()\n      }"
  }
]
```