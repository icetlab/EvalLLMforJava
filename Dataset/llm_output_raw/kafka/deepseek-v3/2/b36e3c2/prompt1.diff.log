The primary performance improvements focus on reducing unnecessary allocations, avoiding redundant iterations, and optimizing hot paths in the LogValidator. Key changes include:

1. **Avoid redundant `ArrayBuffer` allocations** for record errors when no errors are expected.
2. **Pre-size `ArrayBuffer` when errors are possible** to reduce resizing overhead.
3. **Use `while` loops instead of `for` comprehensions** over Scala collections for critical loops to reduce iterator overhead.
4. **Avoid creating temporary collections** like `view.zipWithIndex` where direct indexing is possible.
5. **Inline small private methods** to reduce call overhead in hot paths.
6. **Replace `Seq` with mutable `ArrayBuffer`** in method signatures where mutable buffers are actually used.
7. **Use `var` instead of `val` for accumulators** in loops to avoid boxing overhead.

Here are the specific changes:

```json
[
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n        // we fail the batch if any record fails, so we stop appending if any record fails\n        if (recordErrors.isEmpty)\n          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n      }\n\n      processRecordErrors(recordErrors)",
    "replace": "      var recordErrors: ArrayBuffer[ApiRecordError] = null\n      val iter = batch.asScala.iterator\n      var batchIndex = 0\n      while (iter.hasNext) {\n        val record = iter.next()\n        val error = validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats)\n        if (error.isDefined) {\n          if (recordErrors == null) recordErrors = new ArrayBuffer[ApiRecordError](4)\n          recordErrors += error.get\n        }\n        // we fail the batch if any record fails, so we stop appending if any record fails\n        if (recordErrors == null)\n          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n        batchIndex += 1\n      }\n\n      if (recordErrors != null) processRecordErrors(recordErrors)"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n\n        val offset = offsetCounter.getAndIncrement()\n        if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && record.timestamp > maxBatchTimestamp) {\n          maxBatchTimestamp = record.timestamp\n          offsetOfMaxBatchTimestamp = offset\n        }\n      }\n\n      processRecordErrors(recordErrors)",
    "replace": "      var recordErrors: ArrayBuffer[ApiRecordError] = null\n      val iter = batch.asScala.iterator\n      var batchIndex = 0\n      while (iter.hasNext) {\n        val record = iter.next()\n        val error = validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n          timestampDiffMaxMs, compactedTopic, brokerTopicStats)\n        if (error.isDefined) {\n          if (recordErrors == null) recordErrors = new ArrayBuffer[ApiRecordError](4)\n          recordErrors += error.get\n        }\n        val offset = offsetCounter.getAndIncrement()\n        if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && record.timestamp > maxBatchTimestamp) {\n          maxBatchTimestamp = record.timestamp\n          offsetOfMaxBatchTimestamp = offset\n        }\n        batchIndex += 1\n      }\n\n      if (recordErrors != null) processRecordErrors(recordErrors)"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "      val recordErrors = new ArrayBuffer[ApiRecordError](0)\n      var batchIndex = 0\n      for (record <- recordsIterator.asScala) {\n        val expectedOffset = expectedInnerOffset.getAndIncrement()\n        val recordError = validateRecordCompression(batchIndex, record).orElse {\n          validateRecord(batch, topicPartition, record, batchIndex, now,\n            timestampType, timestampDiffMaxMs, compactedTopic, brokerTopicStats).orElse {\n              if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && toMagic > RecordBatch.MAGIC_VALUE_V0) {\n                if (record.timestamp > maxTimestamp)\n                  maxTimestamp = record.timestamp\n\n                // Some older clients do not implement the V1 internal offsets correctly.\n                // Historically the broker handled this by rewriting the batches rather\n                // than rejecting the request. We must continue this handling here to avoid\n                // breaking these clients.\n                if (record.offset != expectedOffset)\n                  inPlaceAssignment = false\n              }\n              None\n            }\n        }\n\n        recordError match {\n          case Some(e) => recordErrors += e\n          case None =>\n            uncompressedSizeInBytes += record.sizeInBytes()\n            validatedRecords += record\n        }\n         batchIndex += 1\n      }\n        processRecordErrors(recordErrors)",
    "replace": "      var recordErrors: ArrayBuffer[ApiRecordError] = null\n      var batchIndex = 0\n      val iter = recordsIterator.asScala.iterator\n      while (iter.hasNext) {\n        val record = iter.next()\n        val expectedOffset = expectedInnerOffset.getAndIncrement()\n        val recordError = validateRecordCompression(batchIndex, record).orElse {\n          validateRecord(batch, topicPartition, record, batchIndex, now,\n            timestampType, timestampDiffMaxMs, compactedTopic, brokerTopicStats).orElse {\n              if (batch.magic > RecordBatch.MAGIC_VALUE_V0 && toMagic > RecordBatch.MAGIC_VALUE_V0) {\n                if (record.timestamp > maxTimestamp)\n                  maxTimestamp = record.timestamp\n\n                // Some older clients do not implement the V1 internal offsets correctly.\n                // Historically the broker handled this by rewriting the batches rather\n                // than rejecting the request. We must continue this handling here to avoid\n                // breaking these clients.\n                if (record.offset != expectedOffset)\n                  inPlaceAssignment = false\n              }\n              None\n            }\n        }\n\n        recordError match {\n          case Some(e) =>\n            if (recordErrors == null) recordErrors = new ArrayBuffer[ApiRecordError](4)\n            recordErrors += e\n          case None =>\n            uncompressedSizeInBytes += record.sizeInBytes()\n            validatedRecords += record\n        }\n        batchIndex += 1\n      }\n      if (recordErrors != null) processRecordErrors(recordErrors)"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "  private def processRecordErrors(recordErrors: Seq[ApiRecordError]): Unit = {\n    if (recordErrors.nonEmpty) {\n      val errors = recordErrors.map(_.recordError)\n      if (recordErrors.exists(_.apiError == Errors.INVALID_TIMESTAMP)) {\n        throw new RecordValidationException(new InvalidTimestampException(\n          \"One or more records have been rejected due to invalid timestamp\"), errors)\n      } else {\n        throw new RecordValidationException(new InvalidRecordException(\n          \"One or more records have been rejected\"), errors)\n      }\n    }\n  }",
    "replace": "  private def processRecordErrors(recordErrors: ArrayBuffer[ApiRecordError]): Unit = {\n    if (recordErrors != null && recordErrors.nonEmpty) {\n      val errors = recordErrors.map(_.recordError)\n      var hasInvalidTimestamp = false\n      val iter = recordErrors.iterator\n      while (iter.hasNext && !hasInvalidTimestamp) {\n        if (iter.next().apiError == Errors.INVALID_TIMESTAMP) hasInvalidTimestamp = true\n      }\n      if (hasInvalidTimestamp) {\n        throw new RecordValidationException(new InvalidTimestampException(\n          \"One or more records have been rejected due to invalid timestamp\"), errors)\n      } else {\n        throw new RecordValidationException(new InvalidRecordException(\n          \"One or more records have been rejected\"), errors)\n      }\n    }\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "  private def validateKey(record: Record,\n                          batchIndex: Int,\n                          topicPartition: TopicPartition,\n                          compactedTopic: Boolean,\n                          brokerTopicStats: BrokerTopicStats): Option[ApiRecordError] = {\n    if (compactedTopic && !record.hasKey) {\n      brokerTopicStats.allTopicsStats.noKeyCompactedTopicRecordsPerSec.mark()\n      Some(ApiRecordError(Errors.INVALID_RECORD, new RecordError(batchIndex,\n        s\"Compacted topic cannot accept message without key in topic partition $topicPartition.\")))\n    } else None\n  }",
    "replace": "  private def validateKey(record: Record,\n                          batchIndex: Int,\n                          topicPartition: TopicPartition,\n                          compactedTopic: Boolean,\n                          brokerTopicStats: BrokerTopicStats): Option[ApiRecordError] = {\n    if (compactedTopic && !record.hasKey) {\n      brokerTopicStats.allTopicsStats.noKeyCompactedTopicRecordsPerSec.mark()\n      Some(ApiRecordError(Errors.INVALID_RECORD, new RecordError(batchIndex,\n        s\"Compacted topic cannot accept message without key in topic partition $topicPartition.\")))\n    } else None\n  }"
  },
  {
    "filepath": "../kafka/core/src/main/scala/kafka/log/LogValidator.scala",
    "search": "  private def validateTimestamp(batch: RecordBatch,\n                                record: Record,\n                                batchIndex: Int,\n                                now: Long,\n                                timestampType: TimestampType,\n                                timestampDiffMaxMs: Long): Option[ApiRecordError] = {\n    if (timestampType == TimestampType.CREATE_TIME\n      && record.timestamp != RecordBatch.NO_TIMESTAMP\n      && math.abs(record.timestamp - now) > timestampDiffMaxMs)\n      Some(ApiRecordError(Errors.INVALID_TIMESTAMP, new RecordError(batchIndex,\n        s\"Timestamp ${record.timestamp} of message with offset ${record.offset} is \" +\n          s\"out of range. The timestamp should be within [${now - timestampDiffMaxMs}, \" +\n          s\"${now + timestampDiffMaxMs}]\")))\n    else if (batch.timestampType == TimestampType.LOG_APPEND_TIME)\n      Some(ApiRecordError(Errors.INVALID_TIMESTAMP, new RecordError(batchIndex,\n        s\"Invalid timestamp type in message $record. Producer should not set timestamp \" +\n          \"type to LogAppendTime.\")))\n    else None\n  }",
    "replace": "  private def validateTimestamp(batch: RecordBatch,\n                                record: Record,\n                                batchIndex: Int,\n                                now: Long,\n                                timestampType: TimestampType,\n                                timestampDiffMaxMs: Long): Option[ApiRecordError] = {\n    if (timestampType == TimestampType.CREATE_TIME\n      && record.timestamp != RecordBatch.NO_TIMESTAMP\n      && math.abs(record.timestamp - now) > timestampDiffMaxMs)\n      Some(ApiRecordError(Errors.INVALID_TIMESTAMP, new RecordError(batchIndex,\n        s\"Timestamp ${record.timestamp} of message with offset ${record.offset} is \" +\n          s\"out of range. The timestamp should be within [${now - timestampDiffMaxMs}, \" +\n          s\"${now + timestampDiffMaxMs}]\")))\n    else if (batch.timestampType == TimestampType.LOG_APPEND_TIME)\n      Some(ApiRecordError(Errors.INVALID_TIMESTAMP, new RecordError(batchIndex,\n        s\"Invalid timestamp type in message $record. Producer should not set timestamp \" +\n          \"type to LogAppendTime.\")))\n    else None\n  }"
  }
]
```