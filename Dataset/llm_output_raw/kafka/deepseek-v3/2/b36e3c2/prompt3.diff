diff --git a/core/src/main/scala/kafka/log/LogValidator.scala b/core/src/main/scala/kafka/log/LogValidator.scala
index 48b9f57d9a..7931f73981 100644
--- a/core/src/main/scala/kafka/log/LogValidator.scala
+++ b/core/src/main/scala/kafka/log/LogValidator.scala
@@ -237,13 +237,24 @@ private[log] object LogValidator extends Logging {
     for (batch <- records.batches.asScala) {
       validateBatch(topicPartition, firstBatch, batch, origin, toMagicValue, brokerTopicStats)
 
-      val recordErrors = new ArrayBuffer[ApiRecordError](0)
-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {
+      var batchIndex = 0
+      val recordErrors = new ArrayBuffer[ApiRecordError]
+      val recordIter = batch.iterator
+      while (recordIter.hasNext && recordErrors.isEmpty) {
+        val record = recordIter.next()
         validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,
           timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)
-        // we fail the batch if any record fails, so we stop appending if any record fails
-        if (recordErrors.isEmpty)
+        if (recordErrors.isEmpty) {
           builder.appendWithOffset(offsetCounter.getAndIncrement(), record)
+          batchIndex += 1
+        }
+      }
+      // If we stopped early due to errors, we still need to process any remaining records to collect all errors
+      while (recordIter.hasNext) {
+        val record = recordIter.next()
+        validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,
+          timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)
+        batchIndex += 1
       }
 
       processRecordErrors(recordErrors)
@@ -285,8 +296,11 @@ private[log] object LogValidator extends Logging {
       var maxBatchTimestamp = RecordBatch.NO_TIMESTAMP
       var offsetOfMaxBatchTimestamp = -1L
 
-      val recordErrors = new ArrayBuffer[ApiRecordError](0)
-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {
+      var batchIndex = 0
+      val recordErrors = new ArrayBuffer[ApiRecordError]
+      val recordIter = batch.iterator
+      while (recordIter.hasNext) {
+        val record = recordIter.next()
         validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,
           timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)
 
@@ -295,6 +309,7 @@ private[log] object LogValidator extends Logging {
           maxBatchTimestamp = record.timestamp
           offsetOfMaxBatchTimestamp = offset
         }
+        batchIndex += 1
       }
 
       processRecordErrors(recordErrors)
@@ -496,8 +511,9 @@ private[log] object LogValidator extends Logging {
     val builder = MemoryRecords.builder(buffer, magic, compressionType, timestampType, offsetCounter.value,
       logAppendTime, producerId, producerEpoch, baseSequence, isTransactional, partitionLeaderEpoch)
 
-    validatedRecords.foreach { record =>
-      builder.appendWithOffset(offsetCounter.getAndIncrement(), record)
+    val iter = validatedRecords.iterator
+    while (iter.hasNext) {
+      builder.appendWithOffset(offsetCounter.getAndIncrement(), iter.next())
     }
 
     val records = builder.build()
