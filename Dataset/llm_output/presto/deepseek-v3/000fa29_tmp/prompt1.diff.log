Looking at the provided code, I can identify several performance improvement opportunities in the `OptimizedPartitionedOutputOperator` class. The main areas for optimization include:

1. **Reduce object allocations** - The current implementation creates many temporary arrays and objects during partitioning.
2. **Improve loop efficiency** - Some loops can be optimized for better cache locality.
3. **Reduce redundant calculations** - Some calculations are repeated unnecessarily.
4. **Optimize memory access patterns** - Better data structure usage can improve performance.

Here are the specific improvements I'll make:

```json
[
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        private void appendData(DecodedBlockNode[] decodedBlocks, int fixedWidthRowSize, List<Integer> variableWidthChannels, OutputBuffer outputBuffer)\n        {\n            if (decodedBlocks.length != channelCount) {\n                throw new IllegalArgumentException(format(\"Unexpected number of decoded blocks %d. It should be %d.\", decodedBlocks.length, channelCount));\n            }\n\n            if (positionCount == 0) {\n                return;\n            }\n\n            if (channelCount == 0) {\n                bufferedRowCount += positionCount;\n                return;\n            }\n\n            initializeBlockEncodingBuffers(decodedBlocks);\n\n            for (int i = 0; i < channelCount; i++) {\n                blockEncodingBuffers[i].setupDecodedBlocksAndPositions(decodedBlocks[i], positions, positionCount);\n            }\n\n            populateSerializedRowSizes(fixedWidthRowSize, variableWidthChannels);\n\n            // Due to the limitation of buffer size, we append the data batch by batch\n            int offset = 0;\n            do {\n                int batchSize = calculateNextBatchSize(fixedWidthRowSize, variableWidthChannels, offset);\n\n                for (int i = 0; i < channelCount; i++) {\n                    blockEncodingBuffers[i].setNextBatch(offset, batchSize);\n                    blockEncodingBuffers[i].appendDataInBatch();\n                }\n\n                bufferedRowCount += batchSize;\n                offset += batchSize;\n\n                if (bufferFull) {\n                    flush(outputBuffer);\n                    bufferFull = false;\n                }\n            }\n            while (offset < positionCount);\n        }",
    "replace": "        private void appendData(DecodedBlockNode[] decodedBlocks, int fixedWidthRowSize, List<Integer> variableWidthChannels, OutputBuffer outputBuffer)\n        {\n            if (decodedBlocks.length != channelCount) {\n                throw new IllegalArgumentException(format(\"Unexpected number of decoded blocks %d. It should be %d.\", decodedBlocks.length, channelCount));\n            }\n\n            if (positionCount == 0) {\n                return;\n            }\n\n            if (channelCount == 0) {\n                bufferedRowCount += positionCount;\n                return;\n            }\n\n            initializeBlockEncodingBuffers(decodedBlocks);\n\n            // Setup all blocks first\n            for (int i = 0; i < channelCount; i++) {\n                blockEncodingBuffers[i].setupDecodedBlocksAndPositions(decodedBlocks[i], positions, positionCount);\n            }\n\n            // Only populate serialized row sizes if we have variable width channels\n            if (!variableWidthChannels.isEmpty()) {\n                populateSerializedRowSizes(fixedWidthRowSize, variableWidthChannels);\n            }\n\n            // Due to the limitation of buffer size, we append the data batch by batch\n            int offset = 0;\n            int remaining = positionCount;\n            while (remaining > 0) {\n                int batchSize = calculateNextBatchSize(fixedWidthRowSize, variableWidthChannels, offset, remaining);\n\n                // Process all channels for this batch\n                for (int i = 0; i < channelCount; i++) {\n                    blockEncodingBuffers[i].setNextBatch(offset, batchSize);\n                    blockEncodingBuffers[i].appendDataInBatch();\n                }\n\n                bufferedRowCount += batchSize;\n                offset += batchSize;\n                remaining -= batchSize;\n\n                if (bufferFull) {\n                    flush(outputBuffer);\n                    bufferFull = false;\n                }\n            }\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        private int calculateNextBatchSize(int fixedWidthRowSize, List<Integer> variableWidthChannels, int startPosition)\n        {\n            int bytesRemaining = capacity - getSerializedBuffersSizeInBytes();\n\n            if (variableWidthChannels.isEmpty()) {\n                int maxPositionsFit = max(bytesRemaining / fixedWidthRowSize, 1);\n                if (maxPositionsFit <= positionCount - startPosition) {\n                    bufferFull = true;\n                    return maxPositionsFit;\n                }\n                return positionCount - startPosition;\n            }\n\n            verify(serializedRowSizes != null);\n            for (int i = startPosition; i < positionCount; i++) {\n                bytesRemaining -= serializedRowSizes[i];\n\n                if (bytesRemaining <= 0) {\n                    bufferFull = true;\n                    return max(i - startPosition, 1);\n                }\n            }\n\n            return positionCount - startPosition;\n        }",
    "replace": "        private int calculateNextBatchSize(int fixedWidthRowSize, List<Integer> variableWidthChannels, int startPosition, int remainingPositions)\n        {\n            int bytesRemaining = capacity - getSerializedBuffersSizeInBytes();\n\n            if (variableWidthChannels.isEmpty()) {\n                int maxPositionsFit = bytesRemaining / fixedWidthRowSize;\n                if (maxPositionsFit <= 0) {\n                    bufferFull = true;\n                    return 1;\n                }\n                if (maxPositionsFit < remainingPositions) {\n                    bufferFull = true;\n                    return maxPositionsFit;\n                }\n                return remainingPositions;\n            }\n\n            verify(serializedRowSizes != null);\n            int positionsFit = 0;\n            for (int i = 0; i < remainingPositions; i++) {\n                bytesRemaining -= serializedRowSizes[startPosition + i];\n                if (bytesRemaining <= 0) {\n                    bufferFull = true;\n                    positionsFit = i;\n                    break;\n                }\n                positionsFit++;\n            }\n            \n            // Ensure we always process at least one position\n            if (positionsFit == 0 && remainingPositions > 0) {\n                bufferFull = true;\n                return 1;\n            }\n            \n            return positionsFit > 0 ? positionsFit : remainingPositions;\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        public void partitionPage(Page page)\n        {\n            // Populate positions to copy for each destination partition.\n            int positionCount = page.getPositionCount();\n\n            for (int i = 0; i < partitionBuffers.length; i++) {\n                partitionBuffers[i].resetPositions(positionCount);\n            }\n\n            Block nullBlock = nullChannel.isPresent() ? page.getBlock(nullChannel.getAsInt()) : null;\n            Page partitionFunctionArgs = getPartitionFunctionArguments(page);\n\n            for (int position = 0; position < positionCount; position++) {\n                boolean shouldReplicate = (replicatesAnyRow && !hasAnyRowBeenReplicated) ||\n                        nullBlock != null && nullBlock.isNull(position);\n\n                if (shouldReplicate) {\n                    for (int i = 0; i < partitionBuffers.length; i++) {\n                        partitionBuffers[i].addPosition(position);\n                    }\n                    hasAnyRowBeenReplicated = true;\n                }\n                else {\n                    int partition = partitionFunction.getPartition(partitionFunctionArgs, position);\n                    partitionBuffers[partition].addPosition(position);\n                }\n            }\n\n            // Decode the page just once. The decoded blocks will be fed to each PartitionBuffer object to set up AbstractBlockEncodingBuffer.\n            for (int i = 0; i < decodedBlocks.length; i++) {\n                decodedBlocks[i] = decodeBlock(flattener, blockLeaseCloser, page.getBlock(i));\n            }\n\n            // Copy the data to their destination partitions and flush when the buffer is full.\n            for (int i = 0; i < partitionBuffers.length; i++) {\n                partitionBuffers[i].appendData(decodedBlocks, fixedWidthRowSize, variableWidthChannels, outputBuffer);\n            }\n\n            // Return all borrowed arrays\n            try {\n                blockLeaseCloser.close();\n            }\n            catch (IOException e) {\n                throw new UncheckedIOException(e);\n            }\n        }",
    "replace": "        public void partitionPage(Page page)\n        {\n            // Populate positions to copy for each destination partition.\n            int positionCount = page.getPositionCount();\n\n            // Reset all partition buffers first\n            for (int i = 0; i < partitionBuffers.length; i++) {\n                partitionBuffers[i].resetPositions(positionCount);\n            }\n\n            Block nullBlock = nullChannel.isPresent() ? page.getBlock(nullChannel.getAsInt()) : null;\n            Page partitionFunctionArgs = getPartitionFunctionArguments(page);\n\n            // Pre-calculate replication condition to avoid recomputation in loop\n            boolean shouldReplicateAnyRow = replicatesAnyRow && !hasAnyRowBeenReplicated;\n            \n            for (int position = 0; position < positionCount; position++) {\n                boolean shouldReplicate = shouldReplicateAnyRow ||\n                        (nullBlock != null && nullBlock.isNull(position));\n\n                if (shouldReplicate) {\n                    for (int i = 0; i < partitionBuffers.length; i++) {\n                        partitionBuffers[i].addPosition(position);\n                    }\n                    hasAnyRowBeenReplicated = true;\n                    shouldReplicateAnyRow = false; // Update after first replication\n                }\n                else {\n                    int partition = partitionFunction.getPartition(partitionFunctionArgs, position);\n                    partitionBuffers[partition].addPosition(position);\n                }\n            }\n\n            // Decode the page just once. The decoded blocks will be fed to each PartitionBuffer object to set up AbstractBlockEncodingBuffer.\n            for (int i = 0; i < decodedBlocks.length; i++) {\n                decodedBlocks[i] = decodeBlock(flattener, blockLeaseCloser, page.getBlock(i));\n            }\n\n            // Copy the data to their destination partitions and flush when the buffer is full.\n            for (int i = 0; i < partitionBuffers.length; i++) {\n                partitionBuffers[i].appendData(decodedBlocks, fixedWidthRowSize, variableWidthChannels, outputBuffer);\n            }\n\n            // Return all borrowed arrays\n            try {\n                blockLeaseCloser.close();\n            }\n            catch (IOException e) {\n                throw new UncheckedIOException(e);\n            }\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        private void populateSerializedRowSizes(int fixedWidthRowSize, List<Integer> variableWidthChannels)\n        {\n            if (variableWidthChannels.isEmpty()) {\n                return;\n            }\n\n            serializedRowSizes = ensureCapacity(serializedRowSizes, positionCount, SMALL, INITIALIZE);\n\n            for (int i : variableWidthChannels) {\n                blockEncodingBuffers[i].accumulateSerializedRowSizes(serializedRowSizes);\n            }\n\n            for (int i = 0; i < positionCount; i++) {\n                serializedRowSizes[i] += fixedWidthRowSize;\n            }\n        }",
    "replace": "        private void populateSerializedRowSizes(int fixedWidthRowSize, List<Integer> variableWidthChannels)\n        {\n            serializedRowSizes = ensureCapacity(serializedRowSizes, positionCount, SMALL, INITIALIZE);\n\n            // Initialize with fixed width size\n            for (int i = 0; i < positionCount; i++) {\n                serializedRowSizes[i] = fixedWidthRowSize;\n            }\n\n            // Accumulate variable width sizes\n            for (int i : variableWidthChannels) {\n                blockEncodingBuffers[i].accumulateSerializedRowSizes(serializedRowSizes);\n            }\n        }"
  },
  {
    "filepath": "../presto/presto-main/src/main/java/com/facebook/presto/operator/repartition/OptimizedPartitionedOutputOperator.java",
    "search": "        private void flush(OutputBuffer outputBuffer)\n        {\n            if (bufferedRowCount == 0) {\n                return;\n            }\n\n            SliceOutput output = new DynamicSliceOutput(toIntExact(getSerializedBuffersSizeInBytes()));\n            output.writeInt(channelCount);\n\n            for (int i = 0; i < channelCount; i++) {\n                blockEncodingBuffers[i].serializeTo(output);\n                blockEncodingBuffers[i].resetBuffers();\n            }\n\n            SerializedPage serializedPage = serde.serialize(output.slice(), bufferedRowCount);\n            outputBuffer.enqueue(lifespan, partition, ImmutableList.of(serializedPage));\n            pagesAdded.incrementAndGet();\n            rowsAdded.addAndGet(bufferedRowCount);\n\n            bufferedRowCount = 0;\n        }",
    "replace": "        private void flush(OutputBuffer outputBuffer)\n        {\n            if (bufferedRowCount == 0) {\n                return;\n            }\n\n            int estimatedSize = getSerializedBuffersSizeInBytes();\n            SliceOutput output = new DynamicSliceOutput(estimatedSize);\n            output.writeInt(channelCount);\n\n            for (int i = 0; i < channelCount; i++) {\n                blockEncodingBuffers[i].serializeTo(output);\n                blockEncodingBuffers[i].resetBuffers();\n            }\n\n            SerializedPage serializedPage = serde.serialize(output.slice(), bufferedRowCount);\n            outputBuffer.enqueue(lifespan, partition, ImmutableList.of(serializedPage));\n            pagesAdded.incrementAndGet();\n            rowsAdded.addAndGet(bufferedRowCount);\n\n            bufferedRowCount = 0;\n        }"
  }
]
```