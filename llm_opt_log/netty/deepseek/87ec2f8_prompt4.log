Based on the performance issue description and the benchmark, I'll optimize the `decodeString` method in `ByteBufUtil` to reduce unnecessary memory copies by using `new String(...)` directly when possible. Here's the optimized version:

```java
static String decodeString(ByteBuf src, int readerIndex, int len, Charset charset) {
    if (len == 0) {
        return StringUtil.EMPTY_STRING;
    }
    
    // Optimize for heap buffers with array
    if (src.hasArray()) {
        return new String(src.array(), src.arrayOffset() + readerIndex, len, charset);
    }

    // Optimize for direct buffers with single NIO buffer
    if (src.nioBufferCount() == 1) {
        ByteBuffer nioBuffer = src.nioBuffer(readerIndex, len);
        byte[] bytes = new byte[len];
        nioBuffer.get(bytes);
        return new String(bytes, charset);
    }

    // Fall back to original implementation for composite buffers etc.
    final CharsetDecoder decoder = CharsetUtil.decoder(charset);
    final int maxLength = (int) ((double) len * decoder.maxCharsPerByte());
    CharBuffer dst = CHAR_BUFFERS.get();
    if (dst.length() < maxLength) {
        dst = CharBuffer.allocate(maxLength);
        if (maxLength <= MAX_CHAR_BUFFER_SIZE) {
            CHAR_BUFFERS.set(dst);
        }
    } else {
        dst.clear();
    }
    
    // Use a heap buffer as CharsetDecoder is most likely able to use a fast-path if src and dst buffers
    // are both backed by a byte array.
    ByteBuf buffer = src.alloc().heapBuffer(len);
    try {
        buffer.writeBytes(src, readerIndex, len);
        // Use internalNioBuffer(...) to reduce object creation.
        decodeString(decoder, buffer.internalNioBuffer(buffer.readerIndex(), len), dst);
    } finally {
        // Release the temporary buffer again.
        buffer.release();
    }
    return dst.flip().toString();
}
```

Key optimizations made:

1. For heap buffers with backing arrays, we now directly use `new String()` with the underlying byte array, avoiding any intermediate copies or buffer allocations.

2. For direct buffers with a single NIO buffer, we create a byte array and use `new String()` directly, which is more efficient than going through the CharsetDecoder path.

3. Only fall back to the original CharsetDecoder implementation for more complex cases like composite buffers.

This change should significantly improve performance for the common cases while maintaining the same functionality. The optimizations take advantage of Java's built-in string creation optimizations which are highly tuned in the JVM.

The unit tests should continue to pass as the behavior remains the same, just implemented more efficiently. The benchmark should show improved performance, especially for the HEAP and DIRECT buffer types with ASCII and UTF-8 encoding.

Note that I kept the original implementation's handling of character encoding validation and error cases - this is just an optimization of the happy path. The fallback path ensures we maintain all the original behavior for edge cases.